{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/outdated/utils.py:18: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.3.6, the latest is 0.3.8.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  **kwargs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gzip\n",
    "import math\n",
    "import csv\n",
    "import datetime\n",
    "import copy\n",
    "import dateutil.parser\n",
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr, ttest_ind, mannwhitneyu, kruskal, ks_2samp\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import location\n",
    "from utils import Utils\n",
    "from tweet import Tweet\n",
    "from video import Video\n",
    "\n",
    "## [abo | gun | blm]\n",
    "campaign = 'blm'\n",
    "\n",
    "bin_size = 7\n",
    "connection_type = 'followers'\n",
    "year = 2018\n",
    "\n",
    "util = Utils(campaign, bin_size, connection_type, year)\n",
    "\n",
    "tweet = Tweet(util)\n",
    "video = Video(util)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create available early adopters, their tweets, locations, political leanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ea_tweets = tweet.getAvailableTweets(0.2)\n",
    "print('#tweets by available early adopters: {}'.format(len(ea_tweets.keys())))\n",
    "\n",
    "ea_tweets_counts = tweet.getTweetVolumeDistribution(ea_tweets)\n",
    "util.plotLineChart(ea_tweets_counts, \"# of tweets\", \"Weeks\")\n",
    "\n",
    "# Sort tweet counts by weekly\n",
    "ea_tweets_counts_sorted_ind, dates_sorted = util.sortVolumeByDate(ea_tweets_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Methods for tweet-video-user analysis\n",
    "\n",
    "# Analysis of videos by tweet volume within a specific date\n",
    "# tweet.sortVideosByTweetVolumeForSpecificInterval(ea_tweets, '2018-04-15', '2018-04-22')\n",
    "\n",
    "# Print tweets that refer to specific video within a specific week\n",
    "# tweet.printTweetsByVideoId(ea_tweets, '2017-01-01', '2018-05-01', 'Rw9E_nGVXxA')\n",
    "\n",
    "# Analyze tweets of most-tweeted user.\n",
    "# tweet.analyzeTweetsOfMostTweetedUsers(ea_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find early adopters' locations\n",
    "\n",
    "# tweet.assignUserLocations(ea_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find political leanings of seed early adopters.\n",
    "\n",
    "# First find occurrence of hashtags (potential ones for expansion) with seed political hashtags. \n",
    "# hashtags, ht_occur_vec, cooccur_mat = tweet.findHashtagCooccurrences()\n",
    "\n",
    "# Then, manually analyze these hashtags, and decide which hashtags to select for political leaning expansion.\n",
    "# Create 'left_political_hashtags_extended.txt' and 'right_political_hashtags_extended.txt' manually.\n",
    "\n",
    "# Based on extended political hashtags, assign political labels to seed early adopters.\n",
    "# leaning_labels = tweet.assignUserLeaningLabels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#tweet.summarizeUsersTweetsInfo(available_tweets)\n",
    "#tweet.summarizeFollowers()\n",
    "\n",
    "\n",
    "## community summarization\n",
    "#tweet.checkConnectionChanges()\n",
    "#tweet.summarizeCommunities()\n",
    "## (Optional after community detection) check political leanings for each community.\n",
    "# predefined_community_leanings, inferred_community_leanings = tweet.checkLeaningsInCommunities()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign User political leanings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids = tweet.sortVideosByTweetVolumeForSpecificInterval(available_tweets, '2017-01-01', '2018-05-01')\n",
    "video_ids = [video_id[0] for video_id in video_ids]\n",
    "print(len(video_ids))\n",
    "#pickle.dump(video_ids, open('{}_video_ids_from_available_tweets.pkl'.format(util.campaign), 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Assign Video leanings\n",
    "filtered_videos = tweet.getVideoTweetsUsers(available_tweets)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 400\n",
    "# Draw #available_users per video\n",
    "users_per_video = []\n",
    "for video_id in filtered_videos:\n",
    "    users_per_video.append(len(filtered_videos[video_id]['uids']))\n",
    "users_per_video = np.array(users_per_video)\n",
    "users_per_video.sort()\n",
    "#ecdf = ECDF(np.log10(np.cumsum(users_per_video)))\n",
    "ecdf = ECDF(np.log10(users_per_video))\n",
    "plt.plot(ecdf.x, ecdf.y, linestyle=\"--\", linewidth=1.5)\n",
    "plt.xlabel(\"#users (log10)\", fontsize=12)\n",
    "plt.ylabel(\"CDF\", fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Draw #available_tweets per video\n",
    "tweets_per_video = []\n",
    "for video_id in filtered_videos:\n",
    "    tweets_per_video.append(len(filtered_videos[video_id]['tids']))\n",
    "tweets_per_video = np.array(tweets_per_video)\n",
    "tweets_per_video.sort()\n",
    "#ecdf = ECDF(np.log10(np.cumsum(tweets_per_video)))\n",
    "ecdf = ECDF(np.log10(tweets_per_video))\n",
    "plt.plot(ecdf.x, ecdf.y, linestyle=\"--\", linewidth=1.5)\n",
    "plt.xlabel(\"#tweets (log10)\", fontsize=12)\n",
    "plt.ylabel(\"CDF\", fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "\n",
    "video_ids = list(video_leanings_probs.keys())\n",
    "print(len(video_ids))\n",
    "\n",
    "popular_videos_probs_right = []\n",
    "popular_videos_probs_size = []\n",
    "\n",
    "for video_id in video_leanings_probs:\n",
    "    popular_videos_probs_right.append(video_leanings_probs[video_id]['right'])\n",
    "    popular_videos_probs_size.append(len(filtered_videos[video_id]['uids']))\n",
    "\n",
    "# Draw leaning probability (right) and popularity (#users sharing the video)\n",
    "plt.scatter(popular_videos_probs_right, popular_videos_probs_size, c=popular_videos_probs_right, cmap=\"bwr\")\n",
    "plt.xlabel(\"Right leaning prob.\", fontsize=12)\n",
    "plt.ylabel(\"#users promoting the video\", fontsize=12)\n",
    "#plt.legend(prop={'size': 10})\n",
    "plt.show()\n",
    "\n",
    "# Draw probability distribution (right) of video leanings\n",
    "N, bins, patches = pl.hist(popular_videos_probs_right, 20)\n",
    "print('len_patches:', len(patches))\n",
    "jet = pl.get_cmap('bwr', len(patches))\n",
    "for i in range(len(patches)):\n",
    "    patches[i].set_facecolor(jet(i))\n",
    "pl.xlabel('Right leaning prob.', fontsize=12)\n",
    "pl.ylabel('#videos', fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASCADE OPERATIONS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Create sub_followings_dict once to make the cascade computation cost efficient.\n",
    "#tweet.createSubFollowingsDictForCascade()\n",
    "\n",
    "## Calculate cascade for each video in filtered video set.\n",
    "#cascades = tweet.getAllTweetCascades(available_tweets)\n",
    "\n",
    "## Anaylze cascades\n",
    "video_political_leaning_thr = 0.6\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "tweet.analyzeCascadeMeasures(video_leanings_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create min cascade tree for a specific video to plot it.\n",
    "min_user_size = 50\n",
    "max_user_size = 70\n",
    "min_leaning_prob = 0.7\n",
    "max_leaning_prob = 0.9\n",
    "\n",
    "filtered_videos = tweet.getVideoTweetsUsers(available_tweets)\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "video_ids = list(video_leanings_probs.keys())\n",
    "\n",
    "for vid in filtered_videos:\n",
    "    num_users = len(filtered_videos[vid]['uids'])\n",
    "    if num_users >= min_user_size and num_users <= max_user_size:\n",
    "        if video_leanings_probs[vid]['left'] >= min_leaning_prob and video_leanings_probs[vid]['left'] <= max_leaning_prob:\n",
    "            print('Left:', vid, len(filtered_videos[vid]['uids']), video_leanings_probs[vid]['left'])\n",
    "        elif video_leanings_probs[vid]['right'] >= min_leaning_prob and video_leanings_probs[vid]['right'] <= max_leaning_prob:\n",
    "            print('Right:', vid, len(filtered_videos[vid]['uids']), video_leanings_probs[vid]['right'])\n",
    "\n",
    "\n",
    "\n",
    "left_candidate_vid = 'ewLkgfpH6fk' #(68 0.785)\n",
    "left_candidate_vid = 'IYGdeiy0jEw' #(64 0.755)\n",
    "right_candidate_vid = '-R6w07kSjjE' #(65 0.763)\n",
    "        \n",
    "\n",
    "_, cascade_left, users_immediate_neighbors_left = tweet.getTweetCascadeByVideoId(available_tweets, left_candidate_vid)\n",
    "_, cascade_right, users_immediate_neighbors_right = tweet.getTweetCascadeByVideoId(available_tweets, right_candidate_vid)\n",
    "\n",
    "print(cascade_left)\n",
    "\n",
    "print('left cascade for {}:'.format(left_candidate_vid), np.mean([cascade_left[uid]['min'] for uid in cascade_left]), np.median([cascade_left[uid]['min'] for uid in cascade_left]), np.amax([cascade_left[uid]['min'] for uid in cascade_left]))\n",
    "print('right cascade for {}:'.format(right_candidate_vid), np.mean([cascade_right[uid]['min'] for uid in cascade_right]), np.median([cascade_right[uid]['min'] for uid in cascade_right]), np.amax([cascade_right[uid]['min'] for uid in cascade_right]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_video_ids = tweet.getFilteredVideoIds()\n",
    "a = tweet.findEarlyAdopters(0.2, available_tweets, filtered_video_ids)\n",
    "ids = list(a.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENGAGEMENT MEASURE OPERATIONS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate engagement measures for each video in filtered video set.\n",
    "#engagement_measures = tweet.getAllEngagementMeasures(available_tweets)\n",
    "\n",
    "\n",
    "## Anaylze engagement measures for each video in filtered video set.\n",
    "video_political_leaning_thr = 0.6\n",
    "measure_type = 'num_replies'\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "tweet.analyzeEngagementMeasures(measure_type, video_leanings_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRUCTURAL MEASURE OPERATIONS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate structural measures for each video in filtered video set.\n",
    "# nw_type = ['potential' | 'temporal']\n",
    "nw_type = 'potential'\n",
    "#structural_measures = tweet.getAllNetworkStructureMeasures(available_tweets, nw_type)\n",
    "\n",
    "\n",
    "## Anaylze structural measures for each video in filtered video set.\n",
    "#video_political_leaning_thr = 0.6\n",
    "measure_type = 'nw_size'\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "tweet.analyzeStructuralMeasures(measure_type, nw_type, video_leanings_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Analyze cascades  w.r.t. network size\n",
    "video_political_leaning_thr = 0.6\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "\n",
    "median_nw_size = np.median([structural_measures[vid]['nw_size'] for vid in structural_measures])\n",
    "max_nw_size = np.amax([structural_measures[vid]['nw_size'] for vid in structural_measures])\n",
    "print(median_nw_size)\n",
    "print(max_nw_size)\n",
    "\n",
    "video_leanings_probs_for_small_networks = {}\n",
    "video_leanings_probs_for_large_networks = {}\n",
    "for vid in structural_measures:\n",
    "    if structural_measures[vid]['nw_size'] <= median_nw_size:\n",
    "        video_leanings_probs_for_small_networks[vid] = copy.deepcopy(video_leanings_probs[vid])\n",
    "    else:\n",
    "        video_leanings_probs_for_large_networks[vid] = copy.deepcopy(video_leanings_probs[vid])\n",
    "        \n",
    "tweet.analyzeCascadeMeasures(video_leanings_probs_for_large_networks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Analyze the videos with 0 gini (indegree centrality)\n",
    "video_political_leaning_thr = 0.6\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "\n",
    "left_videos = [vid for vid in video_leanings_probs if video_leanings_probs[vid]['left'] > video_political_leaning_thr \n",
    "               and structural_measures[vid]['nw_in_degree_centrality_gini'] == 0]\n",
    "right_videos = [vid for vid in video_leanings_probs if video_leanings_probs[vid]['right'] > video_political_leaning_thr \n",
    "               and structural_measures[vid]['nw_in_degree_centrality_gini'] == 0]\n",
    "neutral_videos = [vid for vid in video_leanings_probs if video_leanings_probs[vid]['right'] <= video_political_leaning_thr \n",
    "                  and video_leanings_probs[vid]['right'] >= (1 - video_political_leaning_thr) \n",
    "                  and structural_measures[vid]['nw_in_degree_centrality_gini'] == 0]\n",
    "\n",
    "print('left videos:', [(vid, structural_measures[vid]['nw_in_degree_centrality_mean']) for vid in left_videos])\n",
    "print('right videos:', [(vid, structural_measures[vid]['nw_in_degree_centrality_mean']) for vid in right_videos])\n",
    "print('neutral videos:', [(vid, structural_measures[vid]['nw_in_degree_centrality_mean']) for vid in neutral_videos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPORAL MEASURE OPERATIONS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate temporal measures for each video in filtered video set.\n",
    "#temporal_measures = tweet.getAllTemporalMeasures(available_tweets)\n",
    "\n",
    "#nw_temporal_diff_wrt_first_tweet_mean, nw_temporal_diff_wrt_first_tweet_median, nw_temporal_diff_between_pairs_mean, nw_diff_speed_mnw_temporal_diff_between_pairs_median, nw_life_time, nw_temporal_diff_between_first_tweets_of_source_users_mean, nw_temporal_diff_between_first_tweets_of_source_users_median, nw_temporal_diff_between_max_indegree_user = tweet.getTemporalMeasuresByVideoId(available_tweets, 'Iqc4BrAzDio')\n",
    "#print(nw_temporal_diff_wrt_first_tweet_mean, nw_temporal_diff_wrt_first_tweet_median, nw_temporal_diff_between_pairs_mean, nw_diff_speed_mnw_temporal_diff_between_pairs_median, nw_life_time, nw_temporal_diff_between_first_tweets_of_source_users_mean, nw_temporal_diff_between_first_tweets_of_source_users_median, nw_temporal_diff_between_max_indegree_user)\n",
    "#'lpskBk0fp4o'\n",
    "#'Iqc4BrAzDio'\n",
    "\n",
    "\n",
    "## Anaylze temporal measures for each video in filtered video set.\n",
    "video_political_leaning_thr = 0.6\n",
    "measure_type = 'nw_temporal_diff_between_max_indegree_user'\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "tweet.analyzeTemporalMeasures(measure_type, video_leanings_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEOGRAPHICAL MEASURE OPERATIONS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Calculate geographical measures for each video in filtered video set.\n",
    "#geo_measures = tweet.getAllNetworkGeographicalMeasures(available_tweets)\n",
    "\n",
    "## Anaylze geographical measures for each video in filtered video set.\n",
    "video_political_leaning_thr = 0.6\n",
    "measure_type = 'nw_locs'\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "left_locs, right_locs, neutral_locs = tweet.analyzeGeographicalMeasures(measure_type, video_leanings_probs)\n",
    "\n",
    "\n",
    "from colour import Color\n",
    "import plotly.graph_objects as go\n",
    "colorscale = [\n",
    "[0, \"#FFFFFF\"], [0.02, \"#F2FFFB\"], [0.04, \"#E8FFF8\"], [0.06, \"#DEFFF5\"], [0.08, \"#D4FFF2\"], [0.1, \"#CAFFEF\"], [0.12, \"#C0FFEB\"], [0.14, \"#B6FFE8\"], [0.16, \"#ACFEE4\"],\n",
    "[0.18, \"#A2FCE0\"], [0.2, \"#98FBDC\"], [0.22, \"#8DF9D8\"], [0.24, \"#84F7D4\"], [0.26, \"#7CF5D0\"], [0.28, \"#73F2CB\"], [0.3, \"#6CEFC7\"], [0.32, \"#64ECC2\"],\n",
    "[0.34, \"#5CE9BE\"], [0.36, \"#55E6B9\"], [0.38, \"#4EE2B4\"], [0.4, \"#48DEAF\"], [0.42, \"#41DAAA\"], [0.44, \"#3BD5A5\"], [0.46, \"#35D0A0\"], [0.48, \"#2FCB9A\"],\n",
    "[0.5, \"#2AC695\"], [0.52, \"#26C392\"], [0.54, \"#22BF8E\"], [0.56, \"#1EBC8A\"], [0.58, \"#1AB886\"], [0.6, \"#16B383\"], [0.62, \"#13AF7F\"], [0.64, \"#10AB7B\"],\n",
    "[0.66, \"#0DA677\"], [0.68, \"#0AA172\"], [0.7, \"#079C6E\"], [0.72, \"#05976A\"], [0.74, \"#039166\"], [0.76, \"#018B61\"], [0.78, \"#00855D\"], [0.8, \"#007F58\"], [0.82, \"#007953\"], [0.84, \"#00734E\"],\n",
    "[0.86, \"#006C49\"], [0.88, \"#006444\"], [0.9, \"#005C3F\"], [0.92, \"#00543A\"], [0.94, \"#004C35\"], [0.96, \"#004330\"], [0.98, \"#003B2A\"], [1, \"#003325\"]\n",
    "]\n",
    "\n",
    "num_colors = 21\n",
    "col_intervals = np.arange(0., 1.001, 1.0/(num_colors-1))\n",
    "white = Color(\"blue\")\n",
    "colors = list(white.range_to(Color(\"red\"), num_colors))\n",
    "print(colors[1], len(colors))\n",
    "colorscale = []\n",
    "for i in range(num_colors):\n",
    "    colorscale.append([col_intervals[i], str(colors[i])])\n",
    "\n",
    "#print(colorscale)\n",
    "\n",
    "state_list = list(location.getStates().keys())\n",
    "\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    #locations=df['code'], # Spatial coordinates\n",
    "    #z = df['total exports'].astype(float), # Data to be color-coded\n",
    "    locations=state_list, # Spatial coordinates\n",
    "    z = [dict(neutral_locs)[loc] for loc in state_list], # Data to be color-coded\n",
    "    locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "    #colorscale = \"rdbu\",\n",
    "    colorscale = \"Reds\",\n",
    "    #reversescale=True,\n",
    "    #colorscale='blues',\n",
    "    #colorbar_title = \"#users\",\n",
    "    colorbar_title = \"Mean user participation (%)\",\n",
    "    #zmin=0.,\n",
    "    #zmax=1.\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'User geo-participation for Neutral Videos',\n",
    "    #title_text = 'Right-wing communities (Gun Control)',\n",
    "    geo_scope='usa', # limite map scope to USA\n",
    "    #geo_scope='north america', # limite map scope to USA\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#fig.write_image(\"abo_right_locs.png\")\n",
    "#pio.write_image(fig, 'images/fig1.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LANGUAGE MEASURE OPERATIONS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate language measures for each video in filtered video set.\n",
    "#language_liwc_measures, language_empath_measures = tweet.getAllLanguageMeasures(available_tweets)\n",
    "\n",
    "\n",
    "## Anaylze language measures for each video in filtered video set.\n",
    "video_political_leaning_thr = 0.6\n",
    "dict_type = 'empath'\n",
    "measure_type = 'heroic'\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "tweet.analyzeLanguageMeasures(dict_type, measure_type, video_leanings_probs)\n",
    "\n",
    "\n",
    "## Anaylze language measures for each video in filtered video set.\n",
    "video_political_leaning_thr = 0.6\n",
    "leaning = 'right'\n",
    "dict_type = 'liwc'\n",
    "measure_type = 'negate'\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "tweet.analyzeTweetsByLanguageCategory(dict_type, measure_type, video_leanings_probs, available_tweets, leaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## VIDEO MEASURE ANALYSIS ##\n",
    "\n",
    "video_political_leaning_thr = 0.6\n",
    "measure_type = 'duration'\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "video_measures = tweet.analyzeVideosByProperty(video_leanings_probs, measure_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## VIDEO MEASURE ANALYSIS (NORMALIZED) ##\n",
    "\n",
    "video_political_leaning_thr = 0.6\n",
    "measure_type = 'commentCount'\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "video_measures = tweet.analyzeVideosByNormalizedProperty(video_leanings_probs, measure_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util.createCandidateVideosFile()\n",
    "#util.createVideosFileForManualAnnotations()\n",
    "#util.createVideosFileForManualAnnotationsUsingSubtitles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIDEO POLARITY, INTENSITY, DIVISIVENESS and POPULARITY ANALYSIS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "video_political_leaning_thr = 0.6\n",
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "video_measures = tweet.analyzeDivisiveContentsByScatterPlots(video_leanings_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "for vid in video_leanings_probs:\n",
    "    print(vid, video_leanings_probs[vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.analyzeAssortativityLeaningRelation(available_tweets, 'tKM3UXstwa0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util.compareAgendasPopulations('num_tweets', 'gun', 'abo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_leanings_probs = tweet.assignVideoLeaningLabels(available_tweets)\n",
    "tweet.separateVideosByLeaning(video_leanings_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
