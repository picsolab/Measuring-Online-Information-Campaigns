{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import csv\n",
    "\n",
    "## [abo | gun | blm]\n",
    "campaign = 'blm'\n",
    "\n",
    "url_tweet_cascade = 'http://130.56.248.5:9357/active-tweets/_search?pretty&scroll=1m'\n",
    "url_tweet_cascade_scroll = 'http://130.56.248.5:9357/_search/scroll?pretty'\n",
    "\n",
    "video_ids = []\n",
    "with open('../data/social_media/{}/video_annotations.csv'.format(campaign)) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        video_id = row[0]\n",
    "        label = row[3]\n",
    "        if label == '1':\n",
    "            video_ids.append(video_id)\n",
    "        line_count += 1\n",
    "    print('Processed {} lines.'.format(line_count))\n",
    "    \n",
    "print('len(video_ids):', len(video_ids))\n",
    "\n",
    "num_tweets_per_video = {}\n",
    "tweets = []\n",
    "total = 0\n",
    "\n",
    "for video_id in video_ids:\n",
    "    scroll_ids = []\n",
    "    query_json = {\n",
    "        \"size\": 5000,\n",
    "        \"query\": {\n",
    "            \"bool\":{\n",
    "                \"should\":[\n",
    "                    {\"match_phrase\": {\"original_vids\": video_id}},\n",
    "                    {\"match_phrase\": {\"retweeted_vids\": video_id}},\n",
    "                    {\"match_phrase\": {\"quoted_vids\": video_id}}\n",
    "                ],\n",
    "                \"minimum_should_match\": 1\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    query_str = json.dumps(query_json)\n",
    "\n",
    "    headers = {'content-type': 'application/json', 'Accept-Charset': 'UTF-8'}\n",
    "\n",
    "    r = requests.post(url_tweet_cascade, data=query_str, headers=headers)\n",
    "    result = json.loads(r.content.decode('utf-8'))\n",
    "    #print(result)\n",
    "    hits = result['hits']['hits']\n",
    "    #print(\"# returned hits:\", len(hits))\n",
    "    num_total_hits = result['hits']['total']\n",
    "    print(video_id)\n",
    "    print(\"# total hits:\", num_total_hits)\n",
    "    num_tweets_per_video[video_id] = num_total_hits\n",
    "    print(\"returned hits: \", len(result['hits']['hits']))\n",
    "    #print(\"max_score:\", result['hits']['max_score'])\n",
    "    last_scroll_id = result['_scroll_id']\n",
    "    scroll_ids.append(last_scroll_id)\n",
    "    total += len(hits)\n",
    "    \n",
    "    for hit in hits:\n",
    "        tweets.append(hit)\n",
    "    \n",
    "    ## Scroll search\n",
    "    while len(hits) > 0:\n",
    "        try:\n",
    "            scroll_query_json = {\n",
    "                \"scroll\" : \"1m\", \n",
    "                \"scroll_id\" : last_scroll_id\n",
    "            }\n",
    "\n",
    "            scroll_query_str = json.dumps(scroll_query_json)\n",
    "            r = requests.post(url_tweet_cascade_scroll, data=scroll_query_str, headers=headers)\n",
    "            result = json.loads(r.content.decode('utf-8'))\n",
    "            hits = result['hits']['hits']\n",
    "            print(\"returned hits:\", len(hits))\n",
    "            total += len(hits)\n",
    "            #print(\"# total:\", total)\n",
    "            last_scroll_id = result['_scroll_id']\n",
    "            scroll_ids.append(last_scroll_id)\n",
    "            for hit in hits:\n",
    "                tweets.append(hit)\n",
    "        except:\n",
    "            print(result)\n",
    "            break\n",
    "    \n",
    "\n",
    "    ## Clear/Close the sessions\n",
    "    clear_scroll_query_json = {\"scroll_id\": scroll_ids}\n",
    "    clear_scroll_query_str = json.dumps(clear_scroll_query_json)\n",
    "    r = requests.delete(url_tweet_cascade_scroll, data=clear_scroll_query_str, headers=headers)\n",
    "\n",
    "    print(\"# tweets:\", len(tweets))\n",
    "    print(\"# total:\", total)\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "results = {}\n",
    "for tweet in tweets:\n",
    "    tweetId = tweet['_source']['tweet_id_str']\n",
    "    results[tweetId] = tweet\n",
    "\n",
    "print(\"# results: \", len(results.keys()))\n",
    "\n",
    "try:\n",
    "    pickle.dump(results, open(\"../data/social_media/{}/all_tweets_by_relevant_videos.pkl\".format(campaign), \"wb\"))\n",
    "except:\n",
    "    print(\"couldn't dump the tweets!!!\")\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
