{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(30)\n",
    "import copy\n",
    "import csv\n",
    "from utils import Utils\n",
    "from operator import itemgetter\n",
    "from tweet import Tweet\n",
    "from video import Video\n",
    "import location\n",
    "from scipy.stats import pearsonr, spearmanr, ttest_ind, mannwhitneyu, kruskal, ks_2samp, spearmanr, pearsonr, sem, t\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "## [abo | gun | blm]\n",
    "campaign = 'abo'\n",
    "\n",
    "bin_size = 7\n",
    "connection_type = 'followers'\n",
    "year = 2018\n",
    "\n",
    "util = Utils(campaign, bin_size, connection_type, year)\n",
    "\n",
    "tweet_obj = Tweet(util)\n",
    "video_obj = Video(util)\n",
    "\n",
    "data_dir = os.path.join('data/social_media/{}/'.format(campaign), 'online_offline_analysis_data')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tweets = pickle.load(open(tweet_obj.initial_tweets_path, 'rb'))\n",
    "num_initial_tweets = len(initial_tweets.keys())\n",
    "initial_users = []\n",
    "for tid in initial_tweets:\n",
    "    uid = initial_tweets[tid]['_source']['user_id_str']\n",
    "    initial_users.append(uid)\n",
    "num_initial_users = len(list(set(initial_users)))\n",
    "print('#initial_tweets: {}, #initial_users: {}'.format(num_initial_tweets, num_initial_users))\n",
    "\n",
    "all_tweets_from_filtered_videos = pickle.load(open(tweet_obj.tweets_path, 'rb'))\n",
    "all_users_from_filtered_videos = pickle.load(open(tweet_obj.users_path, 'rb'))\n",
    "num_mid_tweets = len(all_tweets_from_filtered_videos.keys())\n",
    "mid_users = []\n",
    "for tid in all_tweets_from_filtered_videos:\n",
    "    uid = all_tweets_from_filtered_videos[tid]['_source']['user_id_str']\n",
    "    mid_users.append(uid)\n",
    "num_mid_users = len(list(set(mid_users)))\n",
    "print('#mid_tweets: {}, #mid_users: {}'.format(num_mid_tweets, num_mid_users))\n",
    "print('#mid_users from users: {}'.format(len(all_users_from_filtered_videos.keys())))\n",
    "\n",
    "all_videos_from_annotated_videos = pickle.load(open(tweet_obj.videos_path, 'rb'))\n",
    "filtered_video_ids = tweet_obj.getFilteredVideoIds()\n",
    "num_videos = len(filtered_video_ids)\n",
    "videos = {}\n",
    "total_view_count = 0\n",
    "for vid in filtered_video_ids:\n",
    "    vc = int(all_videos_from_annotated_videos[vid]['_source']['statistics']['viewCount'])\n",
    "    total_view_count += vc\n",
    "print('#videos: {}, #totalView: {}'.format(num_videos, total_view_count))\n",
    "\n",
    "ea_tweets = tweet_obj.getAvailableTweets(0.2)\n",
    "num_ea_tweets = len(ea_tweets.keys())\n",
    "ea_users = []\n",
    "for tid in ea_tweets:\n",
    "    uid = ea_tweets[tid]['_source']['user_id_str']\n",
    "    ea_users.append(uid)\n",
    "num_ea_users = len(list(set(ea_users)))\n",
    "print('#ea_tweets: {}, #ea_users: {}'.format(num_ea_tweets, num_ea_users))\n",
    "\n",
    "ea_tweets_counts = tweet_obj.getTweetVolumeDistribution(ea_tweets)\n",
    "#util.plotLineChart(available_tweets_counts, \"#tweets\", \"weeks\")\n",
    "\n",
    "relevant_videos = {}\n",
    "for vid in filtered_video_ids:\n",
    "    rel_video = copy.deepcopy(all_videos_from_annotated_videos[vid])\n",
    "    relevant_videos[vid] = rel_video\n",
    "relevant_videos_counts = video_obj.getVideoVolumeDistribution(relevant_videos)\n",
    "#util.plotLineChart(available_tweets_counts, \"#videos\", \"weeks\")\n",
    "\n",
    "util.plotTweetVideoChart(ea_tweets_counts, relevant_videos_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare video information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_info = {}\n",
    "ea_videos_tweet_info = {}\n",
    "all_videos_tweet_info = {}\n",
    "for vid in filtered_video_ids:\n",
    "    ea_videos_tweet_info[vid] = {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'users': []}\n",
    "    all_videos_tweet_info[vid] = {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'users': []}\n",
    "\n",
    "video_leanings_probs = tweet_obj.assignVideoLeaningLabels(ea_tweets)\n",
    "\n",
    "all_videos_from_annotated_videos = pickle.load(open(tweet_obj.videos_path, 'rb'))\n",
    "\n",
    "vids = tweet_obj.separateVideosByLeaning(video_leanings_probs)\n",
    "\n",
    "# Got this from Siqi\n",
    "videos_views_120 = pickle.load(open(video_obj.view_120_path, 'rb'))\n",
    "\n",
    "video_first_share_time = {}\n",
    "for tid in ea_tweets:\n",
    "    uid = ea_tweets[tid]['_source']['user_id_str']\n",
    "    tweet_type = None\n",
    "    retweeted_tweet_id_str = ea_tweets[tid]['_source']['retweeted_tweet_id_str']\n",
    "    quoted_tweet_id_str = ea_tweets[tid]['_source']['quoted_tweet_id_str']\n",
    "    reply_user_id_str = ea_tweets[tid]['_source']['reply_user_id_str']\n",
    "    timestamp = int(ea_tweets[tid]['_source']['timestamp_ms'])\n",
    "    original_video_ids = ea_tweets[tid]['_source']['original_vids'].split(';')\n",
    "    retweeted_video_ids = ea_tweets[tid]['_source']['retweeted_vids'].split(';')\n",
    "    quoted_video_ids = ea_tweets[tid]['_source']['quoted_vids'].split(';')\n",
    "    video_ids = list(set(original_video_ids + retweeted_video_ids + quoted_video_ids))\n",
    "    if 'N' in video_ids:\n",
    "        video_ids.remove('N')\n",
    "    \n",
    "    if retweeted_tweet_id_str != None and retweeted_tweet_id_str != 'N':\n",
    "        tweet_type = 'retweet'\n",
    "    elif (retweeted_tweet_id_str == None or retweeted_tweet_id_str == 'N') and (quoted_tweet_id_str != None and quoted_tweet_id_str != 'N'):\n",
    "        tweet_type = 'quoted'\n",
    "    elif reply_user_id_str != None and reply_user_id_str != 'N':\n",
    "        tweet_type = 'reply'\n",
    "    else:\n",
    "        tweet_type = 'original'\n",
    "    \n",
    "    for vid in video_ids:\n",
    "        if vid in filtered_video_ids:\n",
    "            if vid not in video_first_share_time:\n",
    "                video_first_share_time[vid] = timestamp\n",
    "            else:\n",
    "                if timestamp < video_first_share_time[vid]:\n",
    "                    video_first_share_time[vid] = timestamp\n",
    "            \n",
    "            ea_videos_tweet_info[vid][tweet_type] += 1\n",
    "            ea_videos_tweet_info[vid]['users'].append(uid)\n",
    "\n",
    "for tid in all_tweets_from_filtered_videos:\n",
    "    uid = all_tweets_from_filtered_videos[tid]['_source']['user_id_str']\n",
    "    tweet_type = None\n",
    "    retweeted_tweet_id_str = all_tweets_from_filtered_videos[tid]['_source']['retweeted_tweet_id_str']\n",
    "    quoted_tweet_id_str = all_tweets_from_filtered_videos[tid]['_source']['quoted_tweet_id_str']\n",
    "    reply_user_id_str = all_tweets_from_filtered_videos[tid]['_source']['reply_user_id_str']\n",
    "    original_video_ids = all_tweets_from_filtered_videos[tid]['_source']['original_vids'].split(';')\n",
    "    retweeted_video_ids = all_tweets_from_filtered_videos[tid]['_source']['retweeted_vids'].split(';')\n",
    "    quoted_video_ids = all_tweets_from_filtered_videos[tid]['_source']['quoted_vids'].split(';')\n",
    "    video_ids = list(set(original_video_ids + retweeted_video_ids + quoted_video_ids))\n",
    "    if 'N' in video_ids:\n",
    "        video_ids.remove('N')\n",
    "    \n",
    "    if retweeted_tweet_id_str != None and retweeted_tweet_id_str != 'N':\n",
    "        tweet_type = 'retweet'\n",
    "    elif (retweeted_tweet_id_str == None or retweeted_tweet_id_str == 'N') and (quoted_tweet_id_str != None and quoted_tweet_id_str != 'N'):\n",
    "        tweet_type = 'quoted'\n",
    "    elif reply_user_id_str != None and reply_user_id_str != 'N':\n",
    "        tweet_type = 'reply'\n",
    "    else:\n",
    "        tweet_type = 'original'\n",
    "    \n",
    "    for vid in video_ids:\n",
    "        if vid in filtered_video_ids:\n",
    "            all_videos_tweet_info[vid][tweet_type] += 1\n",
    "            all_videos_tweet_info[vid]['users'].append(uid)\n",
    "    \n",
    "for vid in filtered_video_ids:\n",
    "    vid_link = 'https://www.youtube.com/watch?v={}'.format(vid)\n",
    "    vid_leaning_prob = video_leanings_probs[vid]['right']\n",
    "    vid_leaning_label = None\n",
    "    \n",
    "    if vid in vids['R']:\n",
    "        vid_leaning_label = 'R'\n",
    "    elif vid in vids['L']:\n",
    "        vid_leaning_label = 'L'\n",
    "    elif vid in vids['N']:\n",
    "        vid_leaning_label = 'N'\n",
    "    ## May 2018 measures\n",
    "    view_count = int(all_videos_from_annotated_videos[vid]['_source']['statistics']['viewCount'])\n",
    "    like_count = int(all_videos_from_annotated_videos[vid]['_source']['statistics']['likeCount'])\n",
    "    dislike_count = int(all_videos_from_annotated_videos[vid]['_source']['statistics']['dislikeCount'])\n",
    "    comment_count = None\n",
    "    if 'commentCount' in all_videos_from_annotated_videos[vid]['_source']['statistics']:\n",
    "        comment_count = int(all_videos_from_annotated_videos[vid]['_source']['statistics']['commentCount'])\n",
    "    else:\n",
    "        #print(vid, 'commentCount missing!')\n",
    "        comment_count = 'NA'\n",
    "    first_share_time = video_first_share_time[vid]\n",
    "    polarity, intensity, divisiveness, popularity = util.calculateVideoScores(like_count, dislike_count, view_count, first_share_time)\n",
    "    ## first 120 days measures\n",
    "    avg_watch = 60 * all_videos_from_annotated_videos[vid]['_source']['insights']['avgWatch']\n",
    "    duration = util.convertDurationToSeconds(all_videos_from_annotated_videos[vid]['_source']['contentDetails']['duration'])\n",
    "    avg_watch_perc = float(avg_watch)/duration\n",
    "    \n",
    "    view120 = int(videos_views_120[vid]['total_view120'])\n",
    "    \n",
    "    videos_info[vid] = {'id': vid, 'link': vid_link, 'leaning_label': vid_leaning_label, 'leaning_prob': vid_leaning_prob,\n",
    "                        'view_count': view_count, 'like_count': like_count, 'dislike_count': dislike_count, 'comment_count': comment_count, \n",
    "                        'polarity': polarity, 'intensity': intensity, 'divisiveness': divisiveness, 'popularity': popularity, \n",
    "                        'avg_watch': avg_watch, 'duration': duration, 'avg_watch_perc': avg_watch_perc, 'view120': view120\n",
    "                       }\n",
    "    \n",
    "## read structural measures\n",
    "structural_measures = pickle.load(open(tweet_obj.ea_network_structural_measures_path, 'rb'))\n",
    "\n",
    "## read temporal measures\n",
    "temporal_measures = pickle.load(open(tweet_obj.ea_temporal_measures_path, 'rb'))\n",
    "\n",
    "## read language (LIWC) measures\n",
    "language_liwc_calcs = pickle.load(open(tweet_obj.ea_language_liwc_measures_path, 'rb'))\n",
    "language_liwc_measures = {}\n",
    "for vid in filtered_video_ids:\n",
    "    language_liwc_measures[vid] = {}\n",
    "    for measure_type in ['p1', 'p2', 'p3', 'sad', 'anger', 'anx', 'posemo', 'negemo', 'negate', 'wc', 'dic_wc']:\n",
    "        num_tweets_with_measure = None\n",
    "        if measure_type == 'wc' or measure_type == 'dic_wc':\n",
    "            num_tweets_with_measure = language_liwc_calcs[vid][measure_type]\n",
    "        else:\n",
    "            num_tweets_with_measure = len(language_liwc_calcs[vid][measure_type])\n",
    "        language_liwc_measures[vid][measure_type] = float(num_tweets_with_measure) / language_liwc_calcs[vid]['tc']\n",
    "\n",
    "## read cascade measures\n",
    "cascades = pickle.load(open(tweet_obj.ea_cascade_measures_path, 'rb'))\n",
    "cascade_measures = {}\n",
    "for vid in filtered_video_ids:\n",
    "    cascade_measures[vid] = {}\n",
    "    cascade_measures[vid]['mean'] = np.mean([cascades[vid][uid]['min'] for uid in cascades[vid]])\n",
    "    cascade_measures[vid]['median'] = np.median([cascades[vid][uid]['min'] for uid in cascades[vid]])\n",
    "    cascade_measures[vid]['max'] = np.amax([cascades[vid][uid]['min'] for uid in cascades[vid]])\n",
    "    cascade_measures[vid]['num_sources'] = len([cascades[vid][uid]['min'] for uid in cascades[vid] if cascades[vid][uid]['min']==1])\n",
    "\n",
    "\n",
    "print(sum([ea_videos_tweet_info[vid]['original'] for vid in ea_videos_tweet_info]))\n",
    "print(sum([ea_videos_tweet_info[vid]['retweet'] for vid in ea_videos_tweet_info]))\n",
    "print(sum([ea_videos_tweet_info[vid]['quoted'] for vid in ea_videos_tweet_info]))\n",
    "print(sum([ea_videos_tweet_info[vid]['reply'] for vid in ea_videos_tweet_info]))\n",
    "\n",
    "print(sum([all_videos_tweet_info[vid]['original'] for vid in all_videos_tweet_info]))\n",
    "print(sum([all_videos_tweet_info[vid]['retweet'] for vid in all_videos_tweet_info]))\n",
    "print(sum([all_videos_tweet_info[vid]['quoted'] for vid in all_videos_tweet_info]))\n",
    "print(sum([all_videos_tweet_info[vid]['reply'] for vid in all_videos_tweet_info]))\n",
    "\n",
    "with open(os.path.join(data_dir, 'ea_measures.csv'), 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter='\\t')\n",
    "    csv_writer.writerow(['ID', 'Link', 'Leaning Label', 'Leaning Prob', \n",
    "                         '#users (EAs)', '#original_tweets (EAs)', '#retweets (EAs)', '#quoted_tweets (EAs)', '#replies (EAs)', '#total_tweets (EAs)',\n",
    "                         '#users (All)', '#original_tweets (All)', '#retweets (All)', '#quoted_tweets (All)', '#replies (All)', '#total_tweets (All)', \n",
    "                         '#views by May 2018', '#views first 120 days', '#likes by May 2018', '#dislikes by May 2018', '#comments by May 2018', \n",
    "                         'Polarity', 'Intensity', 'Hostility', 'Popularity', \n",
    "                         'AVG(watch time in sec.) for first 120 days', 'AVG(watch percentage) for first 120 days', \n",
    "                         'nw_size', 'nw_max_indegree', 'nw_density', 'nw_in_degree_centrality_gini', 'nw_closeness_centrality_gini', 'nw_betweenness_centrality_gini', \n",
    "                         'nw_global_efficiency', 'nw_degree_assortativity (in-in)', 'nw_assortativity_leaning_probs_abs_nonreciprocal', 'nw_assortativity_leaning_probs_abs_reciprocal', \n",
    "                         'nw_assortativity_leaning_labels', 'nw_assortativity_num_tweets', \n",
    "                         'lang_p1', 'lang_p2', 'lang_p3', 'lang_sad', 'lang_anger', 'lang_anx', 'lang_posemo', 'lang_negemo', \n",
    "                         'lang_negate', 'lang_wc', 'lang_dic_wc', \n",
    "                         'temp_diff_wrt_first_tweet_mean', 'temp_diff_wrt_first_tweet_median', 'temp_diff_between_pairs_mean', \n",
    "                         'temp_diff_between_pairs_median', 'temp_life_time', 'temp_diff_between_first_tweets_of_source_users_mean', 'temp_diff_between_first_tweets_of_source_users_median', \n",
    "                         'temp_diff_between_max_indegree_user', \n",
    "                         'cascade_mean', 'cascade_median', 'cascade_max(depth)', 'cascade_num_sources'])\n",
    "    for vid in filtered_video_ids:\n",
    "        csv_writer.writerow([videos_info[vid]['id'], videos_info[vid]['link'], videos_info[vid]['leaning_label'], videos_info[vid]['leaning_prob'], \n",
    "                             len(set(ea_videos_tweet_info[vid]['users'])), ea_videos_tweet_info[vid]['original'], ea_videos_tweet_info[vid]['retweet'], ea_videos_tweet_info[vid]['quoted'], ea_videos_tweet_info[vid]['reply'], \n",
    "                             (ea_videos_tweet_info[vid]['original'] + ea_videos_tweet_info[vid]['retweet'] + ea_videos_tweet_info[vid]['quoted'] + ea_videos_tweet_info[vid]['reply']),\n",
    "                             len(set(all_videos_tweet_info[vid]['users'])), all_videos_tweet_info[vid]['original'], all_videos_tweet_info[vid]['retweet'], all_videos_tweet_info[vid]['quoted'], all_videos_tweet_info[vid]['reply'], \n",
    "                             (all_videos_tweet_info[vid]['original'] + all_videos_tweet_info[vid]['retweet'] + all_videos_tweet_info[vid]['quoted'] + all_videos_tweet_info[vid]['reply']),\n",
    "                             videos_info[vid]['view_count'], videos_info[vid]['view120'], videos_info[vid]['like_count'], videos_info[vid]['dislike_count'], videos_info[vid]['comment_count'], \n",
    "                             videos_info[vid]['polarity'], videos_info[vid]['intensity'], videos_info[vid]['divisiveness'], videos_info[vid]['popularity'], \n",
    "                             videos_info[vid]['avg_watch'], videos_info[vid]['avg_watch_perc'], \n",
    "                             structural_measures[vid]['nw_size'], structural_measures[vid]['nw_max_indegree'], structural_measures[vid]['nw_density'],\n",
    "                             structural_measures[vid]['nw_in_degree_centrality_gini'], structural_measures[vid]['nw_closeness_centrality_gini'], \n",
    "                             structural_measures[vid]['nw_betweenness_centrality_gini'], structural_measures[vid]['global_efficiency'], \n",
    "                             structural_measures[vid]['nw_degree_assortativity'], structural_measures[vid]['nw_assortativity_leaning_probs_abs_nw1'], structural_measures[vid]['nw_assortativity_leaning_probs_abs_nw2'], \n",
    "                             structural_measures[vid]['nw_assortativity_leaning_labels'], structural_measures[vid]['nw_assortativity_num_tweets'], \n",
    "                             language_liwc_measures[vid]['p1'], language_liwc_measures[vid]['p2'], language_liwc_measures[vid]['p3'], language_liwc_measures[vid]['sad'], \n",
    "                             language_liwc_measures[vid]['anger'], language_liwc_measures[vid]['anx'], language_liwc_measures[vid]['posemo'], language_liwc_measures[vid]['negemo'], \n",
    "                             language_liwc_measures[vid]['negate'], language_liwc_measures[vid]['wc'], language_liwc_measures[vid]['dic_wc'], \n",
    "                             temporal_measures[vid]['nw_temporal_diff_wrt_first_tweet_mean'], temporal_measures[vid]['nw_temporal_diff_wrt_first_tweet_median'], \n",
    "                             temporal_measures[vid]['nw_temporal_diff_between_pairs_mean'], temporal_measures[vid]['nw_diff_speed_mnw_temporal_diff_between_pairs_median'], \n",
    "                             temporal_measures[vid]['nw_life_time'], temporal_measures[vid]['nw_temporal_diff_between_first_tweets_of_source_users_mean'], \n",
    "                             temporal_measures[vid]['nw_temporal_diff_between_first_tweets_of_source_users_median'], temporal_measures[vid]['nw_temporal_diff_between_max_indegree_user'], \n",
    "                             cascade_measures[vid]['mean'], cascade_measures[vid]['median'], cascade_measures[vid]['max'], cascade_measures[vid]['num_sources']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare offline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = sorted(list(location.getStates().keys()))\n",
    "\n",
    "# gun statistics\n",
    "with open('data/offline_statistics/gun_ownership_2015.json') as f:\n",
    "    gun_ownership_2015 = json.load(f)\n",
    "with open('data/offline_statistics/gun_per_capita_2017.json') as f:\n",
    "    gun_per_capita_2017 = json.load(f)\n",
    "with open('data/offline_statistics/gun_per_capita_2018.json') as f:\n",
    "    gun_per_capita_2018 = json.load(f)\n",
    "with open('data/offline_statistics/fatal_injury_by_firearms_all_2017.json') as f:\n",
    "    fatal_injury_by_firearms_all_2017 = json.load(f)\n",
    "with open('data/offline_statistics/fatal_injury_by_firearms_violence_2017.json') as f:\n",
    "    fatal_injury_by_firearms_violence_2017 = json.load(f)\n",
    "with open('data/offline_statistics/fatal_injury_by_firearms_violence_2018.json') as f:\n",
    "    fatal_injury_by_firearms_violence_2018 = json.load(f)\n",
    "with open('data/offline_statistics/fatal_injury_by_firearms_violence_2007_2016.json') as f:\n",
    "    fatal_injury_by_firearms_violence_2007_2016 = json.load(f)\n",
    "with open('data/offline_statistics/federal_firearm_licensees_2017.json') as f:\n",
    "    federal_firearm_licensees_2017 = json.load(f)\n",
    "\n",
    "# abortion statistics\n",
    "with open('data/offline_statistics/abo_public_opinion_illegal_2014.json') as f:\n",
    "    abo_public_opinion_illegal_2014 = json.load(f)\n",
    "with open('data/offline_statistics/abo_public_opinion_legal_2014.json') as f:\n",
    "    abo_public_opinion_legal_2014 = json.load(f)\n",
    "with open('data/offline_statistics/abo_providing_facilities_2017.json') as f:\n",
    "    abo_providing_facilities_2017 = json.load(f)\n",
    "with open('data/offline_statistics/abo_num_abortions_per_1000_women_2017.json') as f:\n",
    "    abo_num_abortions_per_1000_women_2017 = json.load(f)\n",
    "with open('data/offline_statistics/abo_by_state_of_residence_2014.json') as f:\n",
    "    abo_by_state_of_residence_2014 = json.load(f)\n",
    "\n",
    "# BLM statistics\n",
    "with open('data/offline_statistics/blm_protests_2015.json') as f:\n",
    "    blm_protests_2015 = json.load(f)\n",
    "with open('data/offline_statistics/blm_protests_2016.json') as f:\n",
    "    blm_protests_2016 = json.load(f)\n",
    "with open('data/offline_statistics/blm_protests_2017.json') as f:\n",
    "    blm_protests_2017 = json.load(f)\n",
    "with open('data/offline_statistics/blm_protests_2018.json') as f:\n",
    "    blm_protests_2018 = json.load(f)\n",
    "with open('data/offline_statistics/blm_protests_2019.json') as f:\n",
    "    blm_protests_2019 = json.load(f)\n",
    "with open('data/offline_statistics/blm_protests_2020.json') as f:\n",
    "    blm_protests_2020 = json.load(f)\n",
    "with open('data/offline_statistics/blm_support_Apr_2017.json') as f:\n",
    "    blm_support_apr_2017 = json.load(f)\n",
    "with open('data/offline_statistics/blm_support_Aug_2017.json') as f:\n",
    "    blm_support_aug_2017 = json.load(f)\n",
    "with open('data/offline_statistics/blm_support_Jan_2018.json') as f:\n",
    "    blm_support_jan_2018 = json.load(f)\n",
    "with open('data/offline_statistics/blm_support_Jul_2018.json') as f:\n",
    "    blm_support_jul_2018 = json.load(f)\n",
    "with open('data/offline_statistics/blm_support_Jan_2019.json') as f:\n",
    "    blm_support_jan_2019 = json.load(f)\n",
    "with open('data/offline_statistics/blm_support_Jan_2020.json') as f:\n",
    "    blm_support_jan_2020 = json.load(f)\n",
    "with open('data/offline_statistics/blm_oppose_Apr_2017.json') as f:\n",
    "    blm_oppose_apr_2017 = json.load(f)\n",
    "with open('data/offline_statistics/blm_oppose_Aug_2017.json') as f:\n",
    "    blm_oppose_aug_2017 = json.load(f)\n",
    "with open('data/offline_statistics/blm_oppose_Jan_2018.json') as f:\n",
    "    blm_oppose_jan_2018 = json.load(f)\n",
    "with open('data/offline_statistics/blm_oppose_Jul_2018.json') as f:\n",
    "    blm_oppose_jul_2018 = json.load(f)\n",
    "with open('data/offline_statistics/blm_oppose_Jan_2019.json') as f:\n",
    "    blm_oppose_jan_2019 = json.load(f)\n",
    "with open('data/offline_statistics/blm_oppose_Jan_2020.json') as f:\n",
    "    blm_oppose_jan_2020 = json.load(f)\n",
    "\n",
    "\n",
    "with open(os.path.join(data_dir, 'offline_stats.csv'), 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter='\\t')\n",
    "    if campaign == 'abo':\n",
    "        csv_writer.writerow(['State', 'Public Opinion (Should be Legal) (2014)', 'Public Opinion (Should be Ilegal) (2014)', \n",
    "                             'Abortion Providing Facilities (2017)', 'Abortions per 1000 Women (2017)', \n",
    "                             'Abortions by State of Residence (2014)'])\n",
    "        for state in states:\n",
    "            csv_writer.writerow([state, abo_public_opinion_legal_2014[state], abo_public_opinion_illegal_2014[state], \n",
    "                                 abo_providing_facilities_2017[state], abo_num_abortions_per_1000_women_2017[state], \n",
    "                                 abo_by_state_of_residence_2014[state]])\n",
    "    elif campaign == 'gun':\n",
    "        csv_writer.writerow(['State', 'Gun Ownership (2015)', 'Guns per capita (2017)', \n",
    "                             'Fatal Injuries by Firearms (Violence) (2017)', 'Federal Firearm Licensees (2017)',\n",
    "                             'Guns per capita (2018)', 'Fatal Injuries by Firearms (Violence) (2018)', \n",
    "                             'Fatal Injuries by Firearms (Violence) (2007-2016)'])\n",
    "        for state in states:\n",
    "            csv_writer.writerow([state, gun_ownership_2015[state], gun_per_capita_2017[state], \n",
    "                                 fatal_injury_by_firearms_violence_2017[state], federal_firearm_licensees_2017[state], \n",
    "                                 gun_per_capita_2018[state], fatal_injury_by_firearms_violence_2018[state], \n",
    "                                 fatal_injury_by_firearms_violence_2007_2016[state]])\n",
    "    elif campaign == 'blm':\n",
    "        csv_writer.writerow(['State', 'BLM Protests (2015)', 'BLM Protests (2016)', 'BLM Protests (2017)', \n",
    "                             'BLM Protests (2018)', 'BLM Protests (2019)', 'BLM Protests (2020)',\n",
    "                             'BLM Support (Apr 2017)', 'BLM Support (Aug 2017)', 'BLM Support (Jan 2018)',\n",
    "                             'BLM Support (Jul 2018)', 'BLM Support (Jan 2019)', 'BLM Support (Jan 2020)',\n",
    "                             'BLM Oppose (Apr 2017)', 'BLM Oppose (Aug 2017)', 'BLM Oppose (Jan 2018)',\n",
    "                             'BLM Oppose (Jul 2018)', 'BLM Oppose (Jan 2019)', 'BLM Oppose (Jan 2020)'])\n",
    "        for state in states:\n",
    "            csv_writer.writerow([state, blm_protests_2015[state], blm_protests_2016[state], blm_protests_2017[state], \n",
    "                                 blm_protests_2018[state], blm_protests_2019[state], blm_protests_2020[state],\n",
    "                                 blm_support_apr_2017[state], blm_support_aug_2017[state], blm_support_jan_2018[state], \n",
    "                                 blm_support_jul_2018[state], blm_support_jan_2019[state], blm_support_jan_2020[state],\n",
    "                                 blm_oppose_apr_2017[state], blm_oppose_aug_2017[state], blm_oppose_jan_2018[state], \n",
    "                                 blm_oppose_jul_2018[state], blm_oppose_jan_2019[state], blm_oppose_jan_2020[state]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by STATE analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_type: [EA | All]\n",
    "user_type = \"EA\"\n",
    "\n",
    "ea_locs = pickle.load(open(tweet_obj.ea_users_locs_path, 'rb'))\n",
    "all_users_locs = pickle.load(open(tweet_obj.users_locs_path, 'rb'))\n",
    "states = sorted(list(location.getStates().keys()))\n",
    "\n",
    "tweets = ea_tweets if user_type == \"EA\" else all_tweets_from_filtered_videos\n",
    "users_locs = ea_locs if user_type == \"EA\" else all_users_locs\n",
    "video_leanings_probs = tweet_obj.assignVideoLeaningLabels(ea_tweets)\n",
    "\n",
    "print(len(tweets.keys()))\n",
    "\n",
    "vids = tweet_obj.separateVideosByLeaning(video_leanings_probs)\n",
    "\n",
    "virality_scores = {}\n",
    "with open(video_obj.virality_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        vid = row[0]\n",
    "        virality = float(row[1]) * float(row[2])\n",
    "        virality_scores[vid] = virality\n",
    "\n",
    "virality_outlier_vids = ['eUd6Z_zyXZM', 'nJjTpQchohs', 'WULYEegtTGc', 'wZKZ6hpCQqo']\n",
    "\n",
    "\n",
    "c_1_k = {}\n",
    "c_2_k = {}\n",
    "c_all_k = {}\n",
    "for state in states:\n",
    "    c_1_k[state] = {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': [], \n",
    "                    'virality_original': 0, 'virality_retweet': 0, 'virality_quoted': 0, 'virality_reply': 0,\n",
    "                    'virality_total': 0}\n",
    "    c_2_k[state] = {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': [], \n",
    "                    'virality_original': 0, 'virality_retweet': 0, 'virality_quoted': 0, 'virality_reply': 0,\n",
    "                    'virality_total': 0}\n",
    "    c_all_k[state] = {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': [], \n",
    "                      'virality_original': 0, 'virality_retweet': 0, 'virality_quoted': 0, 'virality_reply': 0,\n",
    "                      'virality_total': 0}\n",
    "\n",
    "for tid in tweets:\n",
    "    uid = tweets[tid]['_source']['user_id_str']\n",
    "    user_loc = users_locs[uid]\n",
    "    if user_loc in states:\n",
    "        tweet_type = None\n",
    "        retweeted_tweet_id_str = tweets[tid]['_source']['retweeted_tweet_id_str']\n",
    "        quoted_tweet_id_str = tweets[tid]['_source']['quoted_tweet_id_str']\n",
    "        reply_user_id_str = tweets[tid]['_source']['reply_user_id_str']\n",
    "        original_video_ids = tweets[tid]['_source']['original_vids'].split(';')\n",
    "        retweeted_video_ids = tweets[tid]['_source']['retweeted_vids'].split(';')\n",
    "        quoted_video_ids = tweets[tid]['_source']['quoted_vids'].split(';')\n",
    "        video_ids = list(set(original_video_ids + retweeted_video_ids + quoted_video_ids))\n",
    "        if 'N' in video_ids:\n",
    "            video_ids.remove('N')\n",
    "\n",
    "        if retweeted_tweet_id_str != None and retweeted_tweet_id_str != 'N':\n",
    "            tweet_type = 'retweet'\n",
    "        elif (retweeted_tweet_id_str == None or retweeted_tweet_id_str == 'N') and (quoted_tweet_id_str != None and quoted_tweet_id_str != 'N'):\n",
    "            tweet_type = 'quoted'\n",
    "        elif reply_user_id_str != None and reply_user_id_str != 'N':\n",
    "            tweet_type = 'reply'\n",
    "        else:\n",
    "            tweet_type = 'original'\n",
    "        \n",
    "        virality_type = \"virality_{}\".format(tweet_type)\n",
    "        \n",
    "        for vid in video_ids:\n",
    "            if vid in filtered_video_ids:\n",
    "                if vid in vids['L']:\n",
    "                    c_1_k[user_loc][tweet_type] += 1\n",
    "                    c_1_k[user_loc]['total'] += 1\n",
    "                    c_1_k[user_loc]['users'].append(uid)\n",
    "                    if vid not in virality_outlier_vids:\n",
    "                        c_1_k[user_loc][virality_type] += virality_scores[vid]\n",
    "                        c_1_k[user_loc]['virality_total'] += virality_scores[vid]\n",
    "                elif vid in vids['R']:\n",
    "                    c_2_k[user_loc][tweet_type] += 1\n",
    "                    c_2_k[user_loc]['total'] += 1\n",
    "                    c_2_k[user_loc]['users'].append(uid)\n",
    "                    if vid not in virality_outlier_vids:\n",
    "                        c_2_k[user_loc][virality_type] += virality_scores[vid]\n",
    "                        c_2_k[user_loc]['virality_total'] += virality_scores[vid]\n",
    "                c_all_k[user_loc][tweet_type] += 1\n",
    "                c_all_k[user_loc]['total'] += 1\n",
    "                c_all_k[user_loc]['users'].append(uid)\n",
    "                if vid not in virality_outlier_vids:\n",
    "                    c_all_k[user_loc][virality_type] += virality_scores[vid]\n",
    "                    c_all_k[user_loc]['virality_total'] += virality_scores[vid]\n",
    "\n",
    "p_1_k = {}\n",
    "p_2_k = {}\n",
    "p_0_k = {}\n",
    "for state in states:\n",
    "    p_1_original = float(c_1_k[state]['original'])/c_all_k[state]['original'] if c_all_k[state]['original'] > 0 else np.nan #0\n",
    "    p_1_retweet = float(c_1_k[state]['retweet'])/c_all_k[state]['retweet'] if c_all_k[state]['retweet'] > 0 else np.nan #0\n",
    "    p_1_quoted = float(c_1_k[state]['quoted'])/c_all_k[state]['quoted'] if c_all_k[state]['quoted'] > 0 else np.nan #0\n",
    "    p_1_reply = float(c_1_k[state]['reply'])/c_all_k[state]['reply'] if c_all_k[state]['reply'] > 0 else np.nan #0\n",
    "    p_1_total = float(c_1_k[state]['total'])/c_all_k[state]['total'] if c_all_k[state]['total'] > 0 else np.nan #0\n",
    "    p_1_users = float(len(set(c_1_k[state]['users'])))/len(set(c_all_k[state]['users'])) if len(set(c_all_k[state]['users'])) > 0 else np.nan #0\n",
    "    p_1_virality_original = float(c_1_k[state]['virality_original'])/c_all_k[state]['virality_original'] if c_all_k[state]['virality_original'] > 0 else np.nan #0\n",
    "    p_1_virality_retweet = float(c_1_k[state]['virality_retweet'])/c_all_k[state]['virality_retweet'] if c_all_k[state]['virality_retweet'] > 0 else np.nan #0\n",
    "    p_1_virality_quoted = float(c_1_k[state]['virality_quoted'])/c_all_k[state]['virality_quoted'] if c_all_k[state]['virality_quoted'] > 0 else np.nan #0\n",
    "    p_1_virality_reply = float(c_1_k[state]['virality_reply'])/c_all_k[state]['virality_reply'] if c_all_k[state]['virality_reply'] > 0 else np.nan #0\n",
    "    p_1_virality_total = float(c_1_k[state]['virality_total'])/c_all_k[state]['virality_total'] if c_all_k[state]['virality_total'] > 0 else np.nan #0\n",
    "    \n",
    "    p_2_original = float(c_2_k[state]['original'])/c_all_k[state]['original'] if c_all_k[state]['original'] > 0 else np.nan #0\n",
    "    p_2_retweet = float(c_2_k[state]['retweet'])/c_all_k[state]['retweet'] if c_all_k[state]['retweet'] > 0 else np.nan #0\n",
    "    p_2_quoted = float(c_2_k[state]['quoted'])/c_all_k[state]['quoted'] if c_all_k[state]['quoted'] > 0 else np.nan #0\n",
    "    p_2_reply = float(c_2_k[state]['reply'])/c_all_k[state]['reply'] if c_all_k[state]['reply'] > 0 else np.nan #0\n",
    "    p_2_total = float(c_2_k[state]['total'])/c_all_k[state]['total'] if c_all_k[state]['total'] > 0 else np.nan #0\n",
    "    p_2_users = float(len(set(c_2_k[state]['users'])))/len(set(c_all_k[state]['users'])) if len(set(c_all_k[state]['users'])) > 0 else np.nan #0\n",
    "    p_2_virality_original = float(c_2_k[state]['virality_original'])/c_all_k[state]['virality_original'] if c_all_k[state]['virality_original'] > 0 else np.nan #0\n",
    "    p_2_virality_retweet = float(c_2_k[state]['virality_retweet'])/c_all_k[state]['virality_retweet'] if c_all_k[state]['virality_retweet'] > 0 else np.nan #0\n",
    "    p_2_virality_quoted = float(c_2_k[state]['virality_quoted'])/c_all_k[state]['virality_quoted'] if c_all_k[state]['virality_quoted'] > 0 else np.nan #0\n",
    "    p_2_virality_reply = float(c_2_k[state]['virality_reply'])/c_all_k[state]['virality_reply'] if c_all_k[state]['virality_reply'] > 0 else np.nan #0\n",
    "    p_2_virality_total = float(c_2_k[state]['virality_total'])/c_all_k[state]['virality_total'] if c_all_k[state]['virality_total'] > 0 else np.nan #0\n",
    "    \n",
    "    \n",
    "    p_1_k[state] = {'original': p_1_original,\n",
    "                    'retweet': p_1_retweet, \n",
    "                    'quoted': p_1_quoted, \n",
    "                    'reply': p_1_reply,\n",
    "                    'total': p_1_total,\n",
    "                    'users': p_1_users,\n",
    "                    'virality_original': p_1_virality_original,\n",
    "                    'virality_retweet': p_1_virality_retweet, \n",
    "                    'virality_quoted': p_1_virality_quoted, \n",
    "                    'virality_reply': p_1_virality_reply,\n",
    "                    'virality_total': p_1_virality_total}\n",
    "    p_2_k[state] = {'original': p_2_original,\n",
    "                    'retweet': p_2_retweet, \n",
    "                    'quoted': p_2_quoted, \n",
    "                    'reply': p_2_reply,\n",
    "                    'total': p_2_total,\n",
    "                    'users': p_2_users,\n",
    "                    'virality_original': p_2_virality_original,\n",
    "                    'virality_retweet': p_2_virality_retweet, \n",
    "                    'virality_quoted': p_2_virality_quoted, \n",
    "                    'virality_reply': p_2_virality_reply,\n",
    "                    'virality_total': p_2_virality_total}\n",
    "    \n",
    "    p_0_k[state] = {'original': float(p_1_k[state]['original']) / p_2_k[state]['original'] if p_2_k[state]['original'] > 0 else np.nan, #if p_2_k[state]['original'] != 0 else -1,\n",
    "                    'retweet': float(p_1_k[state]['retweet']) / p_2_k[state]['retweet'] if p_2_k[state]['retweet'] > 0 else np.nan, \n",
    "                    'quoted': float(p_1_k[state]['quoted']) / p_2_k[state]['quoted'] if p_2_k[state]['quoted'] > 0 else np.nan, \n",
    "                    'reply': float(p_1_k[state]['reply']) / p_2_k[state]['reply'] if p_2_k[state]['reply'] > 0 else np.nan,\n",
    "                    'total': float(p_1_k[state]['total']) / p_2_k[state]['total'] if p_2_k[state]['total'] > 0 else np.nan,\n",
    "                    'users': float(p_1_k[state]['users']) / p_2_k[state]['users'] if p_2_k[state]['users'] > 0 else np.nan,\n",
    "                    'virality_original': float(p_1_k[state]['virality_original']) / p_2_k[state]['virality_original'] if p_2_k[state]['virality_original'] > 0 else np.nan, #if p_2_k[state]['original'] != 0 else -1,\n",
    "                    'virality_retweet': float(p_1_k[state]['virality_retweet']) / p_2_k[state]['virality_retweet'] if p_2_k[state]['virality_retweet'] > 0 else np.nan, \n",
    "                    'virality_quoted': float(p_1_k[state]['virality_quoted']) / p_2_k[state]['virality_quoted'] if p_2_k[state]['virality_quoted'] > 0 else np.nan, \n",
    "                    'virality_reply': float(p_1_k[state]['virality_reply']) / p_2_k[state]['virality_reply'] if p_2_k[state]['virality_reply'] > 0 else np.nan,\n",
    "                    'virality_total': float(p_1_k[state]['virality_total']) / p_2_k[state]['virality_total'] if p_2_k[state]['virality_total'] > 0 else np.nan}\n",
    "\n",
    "\n",
    "with open(os.path.join(data_dir, 'by_state_online_measures_{}.csv'.format(user_type.lower())), 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter='\\t')\n",
    "    if user_type == 'EA':\n",
    "        csv_writer.writerow(['State', \n",
    "                             'p_0_k_original_tweets (EAs)', 'p_1_k_original_tweets (EAs)', 'p_2_k_original_tweets (EAs)', \n",
    "                             'p_0_k_retweets (EAs)', 'p_1_k_retweets (EAs)', 'p_2_k_retweets (EAs)', \n",
    "                             'p_0_k_quoted_tweets (EAs)', 'p_1_k_quoted_tweets (EAs)', 'p_2_k_quoted_tweets (EAs)',\n",
    "                             'p_0_k_replies (EAs)', 'p_1_k_replies (EAs)', 'p_2_k_replies (EAs)', \n",
    "                             'p_0_k_total_tweets (EAs)', 'p_1_k_total_tweets (EAs)', 'p_2_k_total_tweets (EAs)', \n",
    "                             'p_0_k_users (EAs)', 'p_1_k_users (EAs)', 'p_2_k_users (EAs)', \n",
    "                             '#users_1_k (EAs)', '#users_2_k (EAs)', '#users_all_k (EAs)',\n",
    "                             'p_0_k_virality_original (EAs)', 'p_1_k_virality_original (EAs)', 'p_2_k_virality_original (EAs)', \n",
    "                             'p_0_k_virality_retweet (EAs)', 'p_1_k_virality_retweet (EAs)', 'p_2_k_virality_retweet (EAs)', \n",
    "                             'p_0_k_virality_quoted (EAs)', 'p_1_k_virality_quoted (EAs)', 'p_2_k_virality_quoted (EAs)',\n",
    "                             'p_0_k_virality_reply (EAs)', 'p_1_k_virality_reply (EAs)', 'p_2_k_virality_reply (EAs)', \n",
    "                             'p_0_k_virality_total (EAs)', 'p_1_k_virality_total (EAs)', 'p_2_k_virality_total (EAs)'])\n",
    "    elif user_type == 'All':\n",
    "        csv_writer.writerow(['State', \n",
    "                             'p_0_k_original_tweets (All)', 'p_1_k_original_tweets (All)', 'p_2_k_original_tweets (All)', \n",
    "                             'p_0_k_retweets (All)', 'p_1_k_retweets (All)', 'p_2_k_retweets (All)', \n",
    "                             'p_0_k_quoted_tweets (All)', 'p_1_k_quoted_tweets (All)', 'p_2_k_quoted_tweets (All)',\n",
    "                             'p_0_k_replies (All)', 'p_1_k_replies (All)', 'p_2_k_replies (All)', \n",
    "                             'p_0_k_total_tweets (All)', 'p_1_k_total_tweets (All)', 'p_2_k_total_tweets (All)', \n",
    "                             'p_0_k_users (All)', 'p_1_k_users (All)', 'p_2_k_users (All)', \n",
    "                             '#users_1_k (All)', '#users_2_k (All)', '#users_all_k (All)',\n",
    "                             'p_0_k_virality_original (All)', 'p_1_k_virality_original (All)', 'p_2_k_virality_original (All)', \n",
    "                             'p_0_k_virality_retweet (All)', 'p_1_k_virality_retweet (All)', 'p_2_k_virality_retweet (All)', \n",
    "                             'p_0_k_virality_quoted (All)', 'p_1_k_virality_quoted (All)', 'p_2_k_virality_quoted (All)',\n",
    "                             'p_0_k_virality_reply (All)', 'p_1_k_virality_reply (All)', 'p_2_k_virality_reply (All)', \n",
    "                             'p_0_k_virality_total (All)', 'p_1_k_virality_total (All)', 'p_2_k_virality_total (All)'])\n",
    "    \n",
    "    for state in states:\n",
    "        csv_writer.writerow([state, \n",
    "                             p_0_k[state]['original'], p_1_k[state]['original'], p_2_k[state]['original'], \n",
    "                             p_0_k[state]['retweet'], p_1_k[state]['retweet'], p_2_k[state]['retweet'],\n",
    "                             p_0_k[state]['quoted'], p_1_k[state]['quoted'], p_2_k[state]['quoted'],\n",
    "                             p_0_k[state]['reply'], p_1_k[state]['reply'], p_2_k[state]['reply'],\n",
    "                             p_0_k[state]['total'], p_1_k[state]['total'], p_2_k[state]['total'],\n",
    "                             p_0_k[state]['users'], p_1_k[state]['users'], p_2_k[state]['users'],\n",
    "                             len(set(c_1_k[state]['users'])), len(set(c_2_k[state]['users'])), len(set(c_all_k[state]['users'])),\n",
    "                             p_0_k[state]['virality_original'], p_1_k[state]['virality_original'], p_2_k[state]['virality_original'], \n",
    "                             p_0_k[state]['virality_retweet'], p_1_k[state]['virality_retweet'], p_2_k[state]['virality_retweet'],\n",
    "                             p_0_k[state]['virality_quoted'], p_1_k[state]['virality_quoted'], p_2_k[state]['virality_quoted'],\n",
    "                             p_0_k[state]['virality_reply'], p_1_k[state]['virality_reply'], p_2_k[state]['virality_reply'],\n",
    "                             p_0_k[state]['virality_total'], p_1_k[state]['virality_total'], p_2_k[state]['virality_total']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by STATE, PARTY analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_leanings_scores = pickle.load(open(tweet_obj.ea_users_inferred_leanings_scores_path, 'rb'))\n",
    "tweets = ea_tweets\n",
    "users_locs = pickle.load(open(tweet_obj.ea_users_locs_path, 'rb'))\n",
    "states = sorted(list(location.getStates().keys()))\n",
    "video_leanings_probs = tweet_obj.assignVideoLeaningLabels(ea_tweets)\n",
    "print(len(tweets.keys()))\n",
    "\n",
    "vids = tweet_obj.separateVideosByLeaning(video_leanings_probs)\n",
    "\n",
    "virality_scores = {}\n",
    "with open(video_obj.virality_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        vid = row[0]\n",
    "        virality = float(row[1]) * float(row[2])\n",
    "        virality_scores[vid] = virality\n",
    "\n",
    "virality_outlier_vids = ['eUd6Z_zyXZM', 'nJjTpQchohs', 'WULYEegtTGc', 'wZKZ6hpCQqo']\n",
    "\n",
    "c_1_k = {}\n",
    "c_2_k = {}\n",
    "for state in states:\n",
    "    c_1_k[state] = {'L': {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': [], \n",
    "                          'virality_original': 0, 'virality_retweet': 0, 'virality_quoted': 0, 'virality_reply': 0, 'virality_total': 0},\n",
    "                    'R': {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': [],\n",
    "                          'virality_original': 0, 'virality_retweet': 0, 'virality_quoted': 0, 'virality_reply': 0, 'virality_total': 0},\n",
    "                    'N': {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': [], \n",
    "                          'virality_original': 0, 'virality_retweet': 0, 'virality_quoted': 0, 'virality_reply': 0, 'virality_total': 0}}\n",
    "    c_2_k[state] = {'L': {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': [], \n",
    "                          'virality_original': 0, 'virality_retweet': 0, 'virality_quoted': 0, 'virality_reply': 0, 'virality_total': 0},\n",
    "                    'R': {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': [], \n",
    "                          'virality_original': 0, 'virality_retweet': 0, 'virality_quoted': 0, 'virality_reply': 0, 'virality_total': 0},\n",
    "                    'N': {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': [], \n",
    "                          'virality_original': 0, 'virality_retweet': 0, 'virality_quoted': 0, 'virality_reply': 0, 'virality_total': 0}}\n",
    "\n",
    "for tid in tweets:\n",
    "    uid = tweets[tid]['_source']['user_id_str']\n",
    "    user_loc = users_locs[uid]\n",
    "    user_party = None\n",
    "    if (user_leanings_scores[uid]['left'] + user_leanings_scores[uid]['right']) != 0:\n",
    "        right_prob = float(user_leanings_scores[uid]['right']) / (user_leanings_scores[uid]['left'] + user_leanings_scores[uid]['right'])\n",
    "        if right_prob > tweet_obj.predefined_video_leaning_thr['right']:\n",
    "            user_party = 'R'\n",
    "        elif right_prob < tweet_obj.predefined_video_leaning_thr['left']:\n",
    "            user_party = 'L'\n",
    "        else:\n",
    "            user_party = 'N'\n",
    "    if user_loc in states and user_party != None:\n",
    "        tweet_type = None\n",
    "        retweeted_tweet_id_str = tweets[tid]['_source']['retweeted_tweet_id_str']\n",
    "        quoted_tweet_id_str = tweets[tid]['_source']['quoted_tweet_id_str']\n",
    "        reply_user_id_str = tweets[tid]['_source']['reply_user_id_str']\n",
    "        original_video_ids = tweets[tid]['_source']['original_vids'].split(';')\n",
    "        retweeted_video_ids = tweets[tid]['_source']['retweeted_vids'].split(';')\n",
    "        quoted_video_ids = tweets[tid]['_source']['quoted_vids'].split(';')\n",
    "        video_ids = list(set(original_video_ids + retweeted_video_ids + quoted_video_ids))\n",
    "        if 'N' in video_ids:\n",
    "            video_ids.remove('N')\n",
    "\n",
    "        if retweeted_tweet_id_str != None and retweeted_tweet_id_str != 'N':\n",
    "            tweet_type = 'retweet'\n",
    "        elif (retweeted_tweet_id_str == None or retweeted_tweet_id_str == 'N') and (quoted_tweet_id_str != None and quoted_tweet_id_str != 'N'):\n",
    "            tweet_type = 'quoted'\n",
    "        elif reply_user_id_str != None and reply_user_id_str != 'N':\n",
    "            tweet_type = 'reply'\n",
    "        else:\n",
    "            tweet_type = 'original'\n",
    "        \n",
    "        virality_type = \"virality_{}\".format(tweet_type)\n",
    "\n",
    "        for vid in video_ids:\n",
    "            if vid in filtered_video_ids:\n",
    "                if vid in vids['L']:\n",
    "                    c_1_k[user_loc][user_party][tweet_type] += 1\n",
    "                    c_1_k[user_loc][user_party]['total'] += 1\n",
    "                    c_1_k[user_loc][user_party]['users'].append(uid)\n",
    "                    if vid not in virality_outlier_vids:\n",
    "                        c_1_k[user_loc][user_party][virality_type] += virality_scores[vid]\n",
    "                        c_1_k[user_loc][user_party]['virality_total'] += virality_scores[vid]\n",
    "                elif vid in vids['R']:\n",
    "                    c_2_k[user_loc][user_party][tweet_type] += 1\n",
    "                    c_2_k[user_loc][user_party]['total'] += 1\n",
    "                    c_2_k[user_loc][user_party]['users'].append(uid)\n",
    "                    if vid not in virality_outlier_vids:\n",
    "                        c_2_k[user_loc][user_party][virality_type] += virality_scores[vid]\n",
    "                        c_2_k[user_loc][user_party]['virality_total'] += virality_scores[vid]\n",
    "\n",
    "p_1_k = {}\n",
    "p_2_k = {}\n",
    "p_0_k = {}\n",
    "for state in states:\n",
    "    p_1_k[state] = {}\n",
    "    p_2_k[state] = {}\n",
    "    p_0_k[state] = {}\n",
    "    for party in [\"L\", \"R\", \"N\"]:\n",
    "        p_1_original = float(c_1_k[state][party]['original'])/sum([c_1_k[state][p]['original'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['original'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_retweet = float(c_1_k[state][party]['retweet'])/sum([c_1_k[state][p]['retweet'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['retweet'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_quoted = float(c_1_k[state][party]['quoted'])/sum([c_1_k[state][p]['quoted'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['quoted'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_reply = float(c_1_k[state][party]['reply'])/sum([c_1_k[state][p]['reply'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['reply'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_total = float(c_1_k[state][party]['total'])/sum([c_1_k[state][p]['total'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['total'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_users = float(len(set(c_1_k[state][party]['users'])))/sum([len(set(c_1_k[state][p]['users'])) for p in c_1_k[state]]) if sum([len(set(c_1_k[state][p]['users'])) for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_virality_original = float(c_1_k[state][party]['virality_original'])/sum([c_1_k[state][p]['virality_original'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['virality_original'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_virality_retweet = float(c_1_k[state][party]['virality_retweet'])/sum([c_1_k[state][p]['virality_retweet'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['virality_retweet'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_virality_quoted = float(c_1_k[state][party]['virality_quoted'])/sum([c_1_k[state][p]['virality_quoted'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['virality_quoted'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_virality_reply = float(c_1_k[state][party]['virality_reply'])/sum([c_1_k[state][p]['virality_reply'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['virality_reply'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        p_1_virality_total = float(c_1_k[state][party]['virality_total'])/sum([c_1_k[state][p]['virality_total'] for p in c_1_k[state]]) if sum([c_1_k[state][p]['virality_total'] for p in c_1_k[state]]) > 0 else np.nan #0\n",
    "        \n",
    "\n",
    "        p_2_original = float(c_2_k[state][party]['original'])/sum([c_2_k[state][p]['original'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['original'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_retweet = float(c_2_k[state][party]['retweet'])/sum([c_2_k[state][p]['retweet'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['retweet'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_quoted = float(c_2_k[state][party]['quoted'])/sum([c_2_k[state][p]['quoted'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['quoted'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_reply = float(c_2_k[state][party]['reply'])/sum([c_2_k[state][p]['reply'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['reply'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_total = float(c_2_k[state][party]['total'])/sum([c_2_k[state][p]['total'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['total'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_users = float(len(set(c_2_k[state][party]['users'])))/sum([len(set(c_2_k[state][p]['users'])) for p in c_2_k[state]]) if sum([len(set(c_2_k[state][p]['users'])) for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_virality_original = float(c_2_k[state][party]['virality_original'])/sum([c_2_k[state][p]['virality_original'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['virality_original'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_virality_retweet = float(c_2_k[state][party]['virality_retweet'])/sum([c_2_k[state][p]['virality_retweet'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['virality_retweet'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_virality_quoted = float(c_2_k[state][party]['virality_quoted'])/sum([c_2_k[state][p]['virality_quoted'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['virality_quoted'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_virality_reply = float(c_2_k[state][party]['virality_reply'])/sum([c_2_k[state][p]['virality_reply'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['virality_reply'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        p_2_virality_total = float(c_2_k[state][party]['virality_total'])/sum([c_2_k[state][p]['virality_total'] for p in c_2_k[state]]) if sum([c_2_k[state][p]['virality_total'] for p in c_2_k[state]]) > 0 else np.nan #0\n",
    "        \n",
    "        \n",
    "        p_1_k[state][party] = {'original': p_1_original,\n",
    "                               'retweet': p_1_retweet, \n",
    "                               'quoted': p_1_quoted, \n",
    "                               'reply': p_1_reply,\n",
    "                               'total': p_1_total,\n",
    "                               'users': p_1_users,\n",
    "                               'virality_original': p_1_virality_original,\n",
    "                               'virality_retweet': p_1_virality_retweet, \n",
    "                               'virality_quoted': p_1_virality_quoted, \n",
    "                               'virality_reply': p_1_virality_reply,\n",
    "                               'virality_total': p_1_virality_total}\n",
    "        p_2_k[state][party] = {'original': p_2_original,\n",
    "                               'retweet': p_2_retweet, \n",
    "                               'quoted': p_2_quoted, \n",
    "                               'reply': p_2_reply,\n",
    "                               'total': p_2_total,\n",
    "                               'users': p_2_users,\n",
    "                               'virality_original': p_2_virality_original,\n",
    "                               'virality_retweet': p_2_virality_retweet, \n",
    "                               'virality_quoted': p_2_virality_quoted, \n",
    "                               'virality_reply': p_2_virality_reply,\n",
    "                               'virality_total': p_2_virality_total}\n",
    "\n",
    "        p_0_k[state][party] = {'original': float(p_1_k[state][party]['original']) / p_2_k[state][party]['original'] if p_2_k[state][party]['original'] > 0 else np.nan, #if p_2_k[state][party]['original'] != 0 else -1,\n",
    "                               'retweet': float(p_1_k[state][party]['retweet']) / p_2_k[state][party]['retweet'] if p_2_k[state][party]['retweet'] > 0 else np.nan, \n",
    "                               'quoted': float(p_1_k[state][party]['quoted']) / p_2_k[state][party]['quoted'] if p_2_k[state][party]['quoted'] > 0 else np.nan, \n",
    "                               'reply': float(p_1_k[state][party]['reply']) / p_2_k[state][party]['reply'] if p_2_k[state][party]['reply'] > 0 else np.nan,\n",
    "                               'total': float(p_1_k[state][party]['total']) / p_2_k[state][party]['total'] if p_2_k[state][party]['total'] > 0 else np.nan,\n",
    "                               'users': float(p_1_k[state][party]['users']) / p_2_k[state][party]['users'] if p_2_k[state][party]['users'] > 0 else np.nan,\n",
    "                               'virality_original': float(p_1_k[state][party]['virality_original']) / p_2_k[state][party]['virality_original'] if p_2_k[state][party]['virality_original'] > 0 else np.nan, #if p_2_k[state][party]['original'] != 0 else -1,\n",
    "                               'virality_retweet': float(p_1_k[state][party]['virality_retweet']) / p_2_k[state][party]['virality_retweet'] if p_2_k[state][party]['virality_retweet'] > 0 else np.nan, \n",
    "                               'virality_quoted': float(p_1_k[state][party]['virality_quoted']) / p_2_k[state][party]['virality_quoted'] if p_2_k[state][party]['virality_quoted'] > 0 else np.nan, \n",
    "                               'virality_reply': float(p_1_k[state][party]['virality_reply']) / p_2_k[state][party]['virality_reply'] if p_2_k[state][party]['virality_reply'] > 0 else np.nan,\n",
    "                               'virality_total': float(p_1_k[state][party]['virality_total']) / p_2_k[state][party]['virality_total'] if p_2_k[state][party]['virality_total'] > 0 else np.nan\n",
    "                               }\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(data_dir, 'by_state_by_party_online_measures.csv'), 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter='\\t')\n",
    "    csv_writer.writerow(['State', \n",
    "                         'p_0_k_original_tweets (L)', 'p_0_k_original_tweets (R)', 'p_1_k_original_tweets (L)', 'p_1_k_original_tweets (R)', 'p_2_k_original_tweets (L)', 'p_2_k_original_tweets (R)', \n",
    "                         'p_0_k_retweets (L)', 'p_0_k_retweets (R)', 'p_1_k_retweets (L)', 'p_1_k_retweets (R)', 'p_2_k_retweets (L)', 'p_2_k_retweets (R)',\n",
    "                         'p_0_k_quoted_tweets (L)', 'p_0_k_quoted_tweets (R)', 'p_1_k_quoted_tweets (L)', 'p_1_k_quoted_tweets (R)', 'p_2_k_quoted_tweets (L)', 'p_2_k_quoted_tweets (R)',\n",
    "                         'p_0_k_replies (L)', 'p_0_k_replies (R)', 'p_1_k_replies (L)', 'p_1_k_replies (R)', 'p_2_k_replies (L)', 'p_2_k_replies (R)', \n",
    "                         'p_0_k_total_tweets (L)', 'p_0_k_total_tweets (R)', 'p_1_k_total_tweets (L)', 'p_1_k_total_tweets (R)', 'p_2_k_total_tweets (L)', 'p_2_k_total_tweets (R)', \n",
    "                         'p_0_k_users (L)', 'p_0_k_users (R)', 'p_1_k_users (L)', 'p_1_k_users (R)', 'p_2_k_users (L)', 'p_2_k_users (R)', \n",
    "                         '#users_1_k (L)', '#users_1_k (R)', '#users_2_k (L)', '#users_2_k (R)', '#users_all_k (L)', '#users_all_k (R)',\n",
    "                         'p_0_k_virality_original (L)', 'p_0_k_virality_original (R)', 'p_1_k_virality_original (L)', 'p_1_k_virality_original (R)', 'p_2_k_virality_original (L)', 'p_2_k_virality_original (R)', \n",
    "                         'p_0_k_virality_retweet (L)', 'p_0_k_virality_retweet (R)', 'p_1_k_virality_retweet (L)', 'p_1_k_virality_retweet (R)', 'p_2_k_virality_retweet (L)', 'p_2_k_virality_retweet (R)',\n",
    "                         'p_0_k_virality_quoted (L)', 'p_0_k_virality_quoted (R)', 'p_1_k_virality_quoted (L)', 'p_1_k_virality_quoted (R)', 'p_2_k_virality_quoted (L)', 'p_2_k_virality_quoted (R)',\n",
    "                         'p_0_k_virality_reply (L)', 'p_0_k_virality_reply (R)', 'p_1_k_virality_reply (L)', 'p_1_k_virality_reply (R)', 'p_2_k_virality_reply (L)', 'p_2_k_virality_reply (R)', \n",
    "                         'p_0_k_virality_total (L)', 'p_0_k_virality_total (R)', 'p_1_k_virality_total (L)', 'p_1_k_virality_total (R)', 'p_2_k_virality_total (L)', 'p_2_k_virality_total (R)'])\n",
    "    for state in states:\n",
    "            csv_writer.writerow([state, \n",
    "                                 p_0_k[state]['L']['original'], p_0_k[state]['R']['original'], p_1_k[state]['L']['original'], p_1_k[state]['R']['original'], p_2_k[state]['L']['original'], p_2_k[state]['R']['original'],\n",
    "                                 p_0_k[state]['L']['retweet'], p_0_k[state]['R']['retweet'], p_1_k[state]['L']['retweet'], p_1_k[state]['R']['retweet'], p_2_k[state]['L']['retweet'], p_2_k[state]['R']['retweet'],\n",
    "                                 p_0_k[state]['L']['quoted'], p_0_k[state]['R']['quoted'], p_1_k[state]['L']['quoted'], p_1_k[state]['R']['quoted'], p_2_k[state]['L']['quoted'], p_2_k[state]['R']['quoted'],\n",
    "                                 p_0_k[state]['L']['reply'], p_0_k[state]['R']['reply'], p_1_k[state]['L']['reply'], p_1_k[state]['R']['reply'], p_2_k[state]['L']['reply'], p_2_k[state]['R']['reply'],\n",
    "                                 p_0_k[state]['L']['total'], p_0_k[state]['R']['total'], p_1_k[state]['L']['total'], p_1_k[state]['R']['total'], p_2_k[state]['L']['total'], p_2_k[state]['R']['total'],\n",
    "                                 p_0_k[state]['L']['users'], p_0_k[state]['R']['users'], p_1_k[state]['L']['users'], p_1_k[state]['R']['users'], p_2_k[state]['L']['users'], p_2_k[state]['R']['users'],\n",
    "                                 len(set(c_1_k[state]['L']['users'])), len(set(c_1_k[state]['R']['users'])), len(set(c_2_k[state]['L']['users'])), len(set(c_2_k[state]['R']['users'])), len(set(c_1_k[state]['L']['users'])) + len(set(c_2_k[state]['L']['users'])), len(set(c_1_k[state]['R']['users'])) + len(set(c_2_k[state]['R']['users'])),\n",
    "                                 p_0_k[state]['L']['virality_original'], p_0_k[state]['R']['virality_original'], p_1_k[state]['L']['virality_original'], p_1_k[state]['R']['virality_original'], p_2_k[state]['L']['virality_original'], p_2_k[state]['R']['virality_original'],\n",
    "                                 p_0_k[state]['L']['virality_retweet'], p_0_k[state]['R']['virality_retweet'], p_1_k[state]['L']['virality_retweet'], p_1_k[state]['R']['virality_retweet'], p_2_k[state]['L']['virality_retweet'], p_2_k[state]['R']['virality_retweet'],\n",
    "                                 p_0_k[state]['L']['virality_quoted'], p_0_k[state]['R']['virality_quoted'], p_1_k[state]['L']['virality_quoted'], p_1_k[state]['R']['virality_quoted'], p_2_k[state]['L']['virality_quoted'], p_2_k[state]['R']['virality_quoted'],\n",
    "                                 p_0_k[state]['L']['virality_reply'], p_0_k[state]['R']['virality_reply'], p_1_k[state]['L']['virality_reply'], p_1_k[state]['R']['virality_reply'], p_2_k[state]['L']['virality_reply'], p_2_k[state]['R']['virality_reply'],\n",
    "                                 p_0_k[state]['L']['virality_total'], p_0_k[state]['R']['virality_total'], p_1_k[state]['L']['virality_total'], p_1_k[state]['R']['virality_total'], p_2_k[state]['L']['virality_total'], p_2_k[state]['R']['virality_total']\n",
    "                                 ])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
