{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(30)\n",
    "import copy\n",
    "import csv\n",
    "from utils import Utils\n",
    "from operator import itemgetter\n",
    "from tweet import Tweet\n",
    "from video import Video\n",
    "import location\n",
    "from scipy.stats import pearsonr, spearmanr, ttest_ind, mannwhitneyu, kruskal, ks_2samp, spearmanr, pearsonr, sem, t\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "## [abo | gun | blm]\n",
    "campaign = 'abo'\n",
    "\n",
    "bin_size = 7\n",
    "connection_type = 'followers'\n",
    "year = 2018\n",
    "\n",
    "util = Utils(campaign, bin_size, connection_type, year)\n",
    "\n",
    "tweet_obj = Tweet(util)\n",
    "video_obj = Video(util)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_from_filtered_videos = pickle.load(open(tweet_obj.tweets_path, 'rb'))\n",
    "all_users_from_filtered_videos = pickle.load(open(tweet_obj.users_path, 'rb'))\n",
    "ea_tweets = tweet_obj.getAvailableTweets(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-cutting Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_leanings_scores = pickle.load(open(tweet_obj.ea_users_inferred_leanings_scores_path, 'rb'))\n",
    "tweets = ea_tweets\n",
    "video_leanings_probs = tweet_obj.assignVideoLeaningLabels(ea_tweets)\n",
    "print(len(tweets.keys()))\n",
    "\n",
    "vids = tweet_obj.separateVideosByLeaning(video_leanings_probs)\n",
    "\n",
    "filtered_video_ids = tweet_obj.getFilteredVideoIds()\n",
    "\n",
    "c_1_k = {}\n",
    "c_2_k = {}\n",
    "for party in [\"L\", \"R\", \"N\"]:\n",
    "    c_1_k[party] = {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': []}\n",
    "    c_2_k[party] = {'original': 0, 'retweet': 0, 'quoted': 0, 'reply': 0, 'total': 0, 'users': []}\n",
    "\n",
    "for tid in tweets:\n",
    "    uid = tweets[tid]['_source']['user_id_str']\n",
    "    user_party = None\n",
    "    if (user_leanings_scores[uid]['left'] + user_leanings_scores[uid]['right']) != 0:\n",
    "        right_prob = float(user_leanings_scores[uid]['right']) / (user_leanings_scores[uid]['left'] + user_leanings_scores[uid]['right'])\n",
    "        if right_prob > tweet_obj.predefined_video_leaning_thr['right']:\n",
    "            user_party = 'R'\n",
    "        elif right_prob < tweet_obj.predefined_video_leaning_thr['left']:\n",
    "            user_party = 'L'\n",
    "        else:\n",
    "            user_party = 'N'\n",
    "         \n",
    "    if user_party != None:\n",
    "        tweet_type = None\n",
    "        retweeted_tweet_id_str = tweets[tid]['_source']['retweeted_tweet_id_str']\n",
    "        quoted_tweet_id_str = tweets[tid]['_source']['quoted_tweet_id_str']\n",
    "        reply_user_id_str = tweets[tid]['_source']['reply_user_id_str']\n",
    "        original_video_ids = tweets[tid]['_source']['original_vids'].split(';')\n",
    "        retweeted_video_ids = tweets[tid]['_source']['retweeted_vids'].split(';')\n",
    "        quoted_video_ids = tweets[tid]['_source']['quoted_vids'].split(';')\n",
    "        video_ids = list(set(original_video_ids + retweeted_video_ids + quoted_video_ids))\n",
    "        if 'N' in video_ids:\n",
    "            video_ids.remove('N')\n",
    "\n",
    "        if retweeted_tweet_id_str != None and retweeted_tweet_id_str != 'N':\n",
    "            tweet_type = 'retweet'\n",
    "        elif (retweeted_tweet_id_str == None or retweeted_tweet_id_str == 'N') and (quoted_tweet_id_str != None and quoted_tweet_id_str != 'N'):\n",
    "            tweet_type = 'quoted'\n",
    "        elif reply_user_id_str != None and reply_user_id_str != 'N':\n",
    "            tweet_type = 'reply'\n",
    "        else:\n",
    "            tweet_type = 'original'\n",
    "\n",
    "        for vid in video_ids:\n",
    "            if vid in filtered_video_ids:\n",
    "                if vid in vids['L']:\n",
    "                    c_1_k[user_party][tweet_type] += 1\n",
    "                    c_1_k[user_party]['total'] += 1\n",
    "                    c_1_k[user_party]['users'].append(uid)\n",
    "                elif vid in vids['R']:\n",
    "                    c_2_k[user_party][tweet_type] += 1\n",
    "                    c_2_k[user_party]['total'] += 1\n",
    "                    c_2_k[user_party]['users'].append(uid)\n",
    "\n",
    "\n",
    "pro_left_original =  float(c_1_k['L']['original'])/sum([c_1_k[p]['original'] for p in c_1_k])\n",
    "pro_left_retweet =   float(c_1_k['L']['retweet'])/sum([c_1_k[p]['retweet'] for p in c_1_k])\n",
    "pro_left_quoted =    float(c_1_k['L']['quoted'])/sum([c_1_k[p]['quoted'] for p in c_1_k])\n",
    "pro_left_reply =     float(c_1_k['L']['reply'])/sum([c_1_k[p]['reply'] for p in c_1_k])\n",
    "pro_left_total =     float(c_1_k['L']['total'])/sum([c_1_k[p]['total'] for p in c_1_k])\n",
    "pro_left_users =     float(len(set(c_1_k['L']['users'])))/sum([len(set(c_1_k[p]['users'])) for p in c_1_k])\n",
    "pro_right_original = float(c_1_k['R']['original'])/sum([c_1_k[p]['original'] for p in c_1_k])\n",
    "pro_right_retweet =  float(c_1_k['R']['retweet'])/sum([c_1_k[p]['retweet'] for p in c_1_k])\n",
    "pro_right_quoted =   float(c_1_k['R']['quoted'])/sum([c_1_k[p]['quoted'] for p in c_1_k])\n",
    "pro_right_reply =    float(c_1_k['R']['reply'])/sum([c_1_k[p]['reply'] for p in c_1_k])\n",
    "pro_right_total =    float(c_1_k['R']['total'])/sum([c_1_k[p]['total'] for p in c_1_k])\n",
    "pro_right_users =    float(len(set(c_1_k['R']['users'])))/sum([len(set(c_1_k[p]['users'])) for p in c_1_k])\n",
    "\n",
    "anti_left_original =  float(c_2_k['L']['original'])/sum([c_2_k[p]['original'] for p in c_2_k])\n",
    "anti_left_retweet =   float(c_2_k['L']['retweet'])/sum([c_2_k[p]['retweet'] for p in c_2_k])\n",
    "anti_left_quoted =    float(c_2_k['L']['quoted'])/sum([c_2_k[p]['quoted'] for p in c_2_k])\n",
    "anti_left_reply =     float(c_2_k['L']['reply'])/sum([c_2_k[p]['reply'] for p in c_2_k])\n",
    "anti_left_total =     float(c_2_k['L']['total'])/sum([c_2_k[p]['total'] for p in c_2_k])\n",
    "anti_left_users =     float(len(set(c_2_k['L']['users'])))/sum([len(set(c_2_k[p]['users'])) for p in c_2_k])\n",
    "anti_right_original = float(c_2_k['R']['original'])/sum([c_2_k[p]['original'] for p in c_2_k])\n",
    "anti_right_retweet =  float(c_2_k['R']['retweet'])/sum([c_2_k[p]['retweet'] for p in c_2_k])\n",
    "anti_right_quoted =   float(c_2_k['R']['quoted'])/sum([c_2_k[p]['quoted'] for p in c_2_k])\n",
    "anti_right_reply =    float(c_2_k['R']['reply'])/sum([c_2_k[p]['reply'] for p in c_2_k])\n",
    "anti_right_total =    float(c_2_k['R']['total'])/sum([c_2_k[p]['total'] for p in c_2_k])\n",
    "anti_right_users =    float(len(set(c_2_k['R']['users'])))/sum([len(set(c_2_k[p]['users'])) for p in c_2_k])\n",
    "\n",
    "print(pro_right_original, pro_right_retweet, pro_right_quoted, pro_right_reply, pro_right_total, pro_right_users)\n",
    "print(anti_left_original, anti_left_retweet, anti_left_quoted, anti_left_reply, anti_left_total, anti_left_users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find example videos and networks based on nw_closeness_centrality_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read structural measures\n",
    "structural_measures = pickle.load(open(tweet_obj.ea_network_structural_measures_path, 'rb'))\n",
    "video_leanings_probs = tweet_obj.assignVideoLeaningLabels(ea_tweets)\n",
    "tmp_video_leanings_probs = {}\n",
    "for vid in video_leanings_probs:\n",
    "    tmp_video_leanings_probs[vid] = video_leanings_probs[vid]['right']\n",
    "\n",
    "sorted_video_leanings_probs = sorted(tmp_video_leanings_probs.items(), key=itemgetter(1))\n",
    "for pair in sorted_video_leanings_probs:\n",
    "    print(pair, structural_measures[pair[0]]['nw_closeness_centrality_gini'], structural_measures[pair[0]]['nw_size'])\n",
    "\n",
    "sub_followings_dict = pickle.load(open(tweet_obj.ea_users_sub_followings_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create network graph and visualize it\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "\n",
    "#vid = 'ewLkgfpH6fk'\n",
    "vid = '5CoxmjwFn2Q'\n",
    "\n",
    "uids = []\n",
    "for tid in ea_tweets:\n",
    "    uid = ea_tweets[tid]['_source']['user_id_str']\n",
    "    original_video_ids = ea_tweets[tid]['_source']['original_vids'].split(';')\n",
    "    retweeted_video_ids = ea_tweets[tid]['_source']['retweeted_vids'].split(';')\n",
    "    quoted_video_ids = ea_tweets[tid]['_source']['quoted_vids'].split(';')\n",
    "    video_ids = list(set(original_video_ids + retweeted_video_ids + quoted_video_ids))\n",
    "    if 'N' in video_ids:\n",
    "        video_ids.remove('N')\n",
    "    if vid in video_ids:\n",
    "        uids.append(uid)\n",
    "uids = list(set(uids))\n",
    "print('len(uids):', len(uids))\n",
    "\n",
    "## create graph\n",
    "G = ig.Graph(directed=True)\n",
    "G.add_vertices(len(uids))\n",
    "for uid in uids:\n",
    "    for fid in sub_followings_dict[uid]:\n",
    "        if fid in uids and fid != uid:\n",
    "            G.add_edges([(uids.index(uid), uids.index(fid))])\n",
    "G.degree(mode=\"out\")\n",
    "\n",
    "print('G:', G.vcount(), G.ecount())\n",
    "\n",
    "A = G.get_edgelist()\n",
    "G2 = nx.DiGraph(A) # In case your graph is directed\n",
    "for uid in uids:\n",
    "    if uids.index(uid) not in G2.nodes:\n",
    "        G2.add_node(uids.index(uid))\n",
    "closeness_centralities = nx.algorithms.closeness_centrality(G2, wf_improved=True)\n",
    "nw_closeness_centrality_gini = tweet_obj.util.gini(list(closeness_centralities.values()))\n",
    "print('G2:', G2.number_of_nodes(), G2.number_of_edges())\n",
    "print(nw_closeness_centrality_gini)\n",
    "\n",
    "\n",
    "# visualize the network\n",
    "for nid in range(len(uids)):\n",
    "    G.vs[nid][\"size\"] = 100\n",
    "    G.vs[nid][\"color\"] = 'white'\n",
    "    G.vs[nid][\"label\"] = \"U{}\".format(nid)\n",
    "    G.vs[nid][\"label_size\"] = 35\n",
    "\n",
    "visual_style = {}\n",
    "visual_style[\"layout\"] = G.layout_star(center=27)\n",
    "visual_style[\"edge_width\"] = 2\n",
    "visual_style[\"bbox\"] = (3500, 3500)\n",
    "ig.plot(G, \"deneme_{}.pdf\".format(vid), **visual_style)\n",
    "\n",
    "'''\n",
    "layout = []\n",
    "\n",
    "x = -1\n",
    "y = 0\n",
    "for i in range(len(uids)):\n",
    "    layout.append((x, y))\n",
    "    x += 0.05\n",
    "\n",
    "visual_style = {}\n",
    "visual_style[\"bbox\"] = (5000, 1000)\n",
    "visual_style[\"layout\"] = layout\n",
    "visual_style[\"edge_curved\"] = 0.3\n",
    "visual_style[\"vertex_label_size\"] = 24\n",
    "visual_style[\"margin\"] = 250\n",
    "#visual_style[\"edge_width\"] = [float(width)/100 for width in edge_weights]\n",
    "ig.plot(G, \"deneme_lll.pdf\", **visual_style)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample videos based on their score percentile for sanity check of leaning assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_leanings_probs = tweet_obj.assignVideoLeaningLabels(ea_tweets)\n",
    "vids = list(video_leanings_probs.keys())\n",
    "scores = [video_leanings_probs[vid]['right'] for vid in vids]\n",
    "\n",
    "def sampleVideosByScore(scores):\n",
    "    cuts = np.linspace(0, 100, 11)\n",
    "    results = []\n",
    "    for i in range(len(cuts)-1):\n",
    "        results.append([])\n",
    "        lower = np.percentile(scores, cuts[i])\n",
    "        upper = np.percentile(scores, cuts[i+1])\n",
    "        perc_scores_inds = np.where((scores>=lower) & (scores<upper))[0]\n",
    "        rndm_perc_scores_inds = random.sample(list(perc_scores_inds), k=5)\n",
    "        for ind in rndm_perc_scores_inds:\n",
    "            results[i].append(vids[ind])\n",
    "    return results\n",
    "\n",
    "result = sampleVideosByScore(scores)\n",
    "\n",
    "ind = 8\n",
    "cuts = np.linspace(0, 100, 11)\n",
    "print(cuts)\n",
    "#print(cuts[ind], cuts[ind+1])\n",
    "print(np.percentile(scores, cuts[ind]), np.percentile(scores, cuts[ind+1]))\n",
    "for vid in result[ind]:\n",
    "    print(vid, video_leanings_probs[vid]['right'])\n",
    "\n",
    "aa = []\n",
    "for i in range(len(result)):\n",
    "    for j in range(len(result[i])):\n",
    "        aa.append(result[i][j])\n",
    "\n",
    "print(len(aa), len(set(aa)))\n",
    "random.shuffle(aa)\n",
    "\n",
    "for vid in aa:\n",
    "    print(vid, 'https://www.youtube.com/watch?v='+vid, video_leanings_probs[vid]['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
